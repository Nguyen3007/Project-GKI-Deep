{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oLxgpls6PH2i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import Counter\n",
        "import zipfile\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "zip_path = \"/content/vintext.zip\"\n",
        "extract_path = \"/content/vintext\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Đã giải nén VinText.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbwNg4c0TrNU",
        "outputId": "aee1c7b6-56ac-4ddc-d345-e37f7039f43e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đã giải nén VinText.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "LABEL_DIR = '/content/vintext/labels'\n",
        "\n",
        "def collect_characters(label_dir):\n",
        "    charset = set()\n",
        "    for label_file in glob.glob(os.path.join(label_dir, 'gt_*.txt')):\n",
        "        with open(label_file, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(',')\n",
        "                if len(parts) < 9:\n",
        "                    continue\n",
        "                text = ','.join(parts[8:]).strip().lower()\n",
        "                if text == '###' or len(text) < 2:\n",
        "                    continue\n",
        "                for ch in text:\n",
        "                    charset.add(ch)\n",
        "    return charset\n",
        "\n",
        "def write_vocab(charset, save_path='/content/vintext/vocab.txt'):\n",
        "    # Thêm các token đặc biệt\n",
        "    tokens = ['<PAD>', '<BOS>', '<EOS>', '<UNK>']\n",
        "    # Sắp xếp charset theo unicode để cố định thứ tự\n",
        "    charset = sorted(list(charset))\n",
        "    with open(save_path, 'w', encoding='utf-8') as f:\n",
        "        for token in tokens:\n",
        "            f.write(token + '\\n')\n",
        "        for ch in charset:\n",
        "            f.write(ch + '\\n')\n",
        "    print(f'Vocab saved to {save_path}. Total chars (not count special tokens): {len(charset)}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    charset = collect_characters(LABEL_DIR)\n",
        "    write_vocab(charset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0Pe1sovVIU8",
        "outputId": "41b2c26b-ae66-4d04-f80f-0ca3155c582c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab saved to /content/vintext/vocab.txt. Total chars (not count special tokens): 126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, vocab_path):\n",
        "        self.idx2char = []\n",
        "        with open(vocab_path, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                self.idx2char.append(line.strip('\\n'))\n",
        "        self.char2idx = {ch: idx for idx, ch in enumerate(self.idx2char)}\n",
        "        self.pad_idx = self.char2idx['<PAD>']\n",
        "        self.bos_idx = self.char2idx['<BOS>']\n",
        "        self.eos_idx = self.char2idx['<EOS>']\n",
        "        self.unk_idx = self.char2idx['<UNK>']\n",
        "\n",
        "    def encode(self, text, max_length):\n",
        "        ids = [self.bos_idx]\n",
        "        for ch in text:\n",
        "            ids.append(self.char2idx.get(ch, self.unk_idx))\n",
        "        ids.append(self.eos_idx)\n",
        "        # Pad to max_length\n",
        "        ids = ids[:max_length]\n",
        "        ids += [self.pad_idx] * (max_length - len(ids))\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        chars = []\n",
        "        for idx in ids:\n",
        "            ch = self.idx2char[idx]\n",
        "            if ch in ['<PAD>', '<BOS>', '<EOS>']:\n",
        "                continue\n",
        "            chars.append(ch)\n",
        "        return ''.join(chars)\n",
        "\n",
        "class VinTextOCREnd2EndDataset(Dataset):\n",
        "    def __init__(self, img_dir, label_dir, vocab_path, max_label_length=128, img_height=64, img_width=256, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.vocab = Vocab(vocab_path)\n",
        "        self.max_label_length = max_label_length\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.transform = transform or transforms.Compose([\n",
        "            transforms.Resize((img_height, img_width)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        self.samples = []\n",
        "        label_files = sorted(glob.glob(os.path.join(label_dir, \"gt_*.txt\")))\n",
        "        for label_file in label_files:\n",
        "            img_id = os.path.splitext(os.path.basename(label_file))[0][3:]  # gt_1.txt -> '1'\n",
        "            img_name = f\"im{int(img_id):04d}.jpg\"\n",
        "            img_path = os.path.join(img_dir, img_name)\n",
        "            if not os.path.exists(img_path):\n",
        "                continue\n",
        "            with open(label_file, encoding='utf-8') as f:\n",
        "                texts = []\n",
        "                for line in f:\n",
        "                    parts = line.strip().split(',')\n",
        "                    if len(parts) < 9:\n",
        "                        continue\n",
        "                    text = ','.join(parts[8:]).strip().lower()\n",
        "                    if text == '###' or len(text) < 2:\n",
        "                        continue\n",
        "                    texts.append(text)\n",
        "                if texts:\n",
        "                    merged_text = ' '.join(texts)\n",
        "                    self.samples.append((img_path, merged_text))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, text = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        label_ids = self.vocab.encode(text, self.max_label_length)\n",
        "        label_ids = torch.tensor(label_ids, dtype=torch.long)\n",
        "        return img, label_ids, text  # trả về cả text gốc để debug dễ dàng\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    img_dir = \"/content/vintext/train_images\"\n",
        "    label_dir = \"/content/vintext/labels\"\n",
        "    vocab_path = \"/content/vintext/vocab.txt\"\n",
        "    dataset = VinTextOCREnd2EndDataset(img_dir, label_dir, vocab_path)\n",
        "    print(\"Samples:\", len(dataset))\n",
        "    img, label_ids, gt_text = dataset[0]\n",
        "    print(\"Image shape:\", img.shape)\n",
        "    print(\"Label ids:\", label_ids)\n",
        "    print(\"Decoded:\", dataset.vocab.decode(label_ids.tolist()))\n",
        "    print(\"Groundtruth text:\", gt_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3BGXNe_g1Zp",
        "outputId": "7a8dd844-0dd3-4a84-e333-f8d241bf0909"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples: 1198\n",
            "Image shape: torch.Size([3, 64, 256])\n",
            "Label ids: tensor([  1,  39,  44,  88,  56,   4,  48,  85, 119,  50,  43,   4,  56, 110,\n",
            "         56,   4,  81, 103,   4,  39,  74,   4,  58,  45, 105,  39,   4,  48,\n",
            "         64,  49,  15,   4,  50,  80,  50,  43,   4,  55,  57,  88,  56,   4,\n",
            "         39,  37,  51,   4,  81, 103,   4,  56,  80,  50,  43,   4,  56,  44,\n",
            "         57,   4,  50,  44,  92,  52,   2,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0])\n",
            "Decoded: chất lượng tốt để có việc làm, năng suất cao để tăng thu nhập\n",
            "Groundtruth text: chất lượng tốt để có việc làm, năng suất cao để tăng thu nhập\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CNNBackbone(nn.Module):\n",
        "    def __init__(self, out_channels=256):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "        self.conv_layers = nn.Sequential(*list(resnet.children())[:-2])\n",
        "        self.proj = nn.Conv2d(resnet.fc.in_features, out_channels, 1)\n",
        "    def forward(self, x):\n",
        "        feat = self.conv_layers(x)\n",
        "        feat = self.proj(feat)\n",
        "        return feat"
      ],
      "metadata": {
        "id": "bHMAkvxohHV3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0))/d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]"
      ],
      "metadata": {
        "id": "5jQC_M1YhVaM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OCRTransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=3, dropout=0.1, max_len=48):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embed = PositionalEncoding(d_model, max_len)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, d_model*2, dropout)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "    def forward(self, memory, tgt, tgt_mask=None):\n",
        "        tgt_emb = self.token_embed(tgt) * (self.d_model ** 0.5)\n",
        "        tgt_emb = self.pos_embed(tgt_emb)\n",
        "        tgt_emb = tgt_emb.transpose(0,1) # (tgt_len,B,E)\n",
        "        memory = memory.transpose(0,1)\n",
        "        output = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
        "        output = output.transpose(0,1)   # (B, tgt_len, E)\n",
        "        logits = self.output_proj(output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "R2kajEizhdKV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OCRNet(nn.Module):\n",
        "    def __init__(self, vocab_size, max_seq_len=48, d_model=256):\n",
        "        super().__init__()\n",
        "        self.cnn = CNNBackbone(out_channels=d_model)\n",
        "        self.feat2seq = nn.Linear(d_model, d_model)\n",
        "        self.decoder = OCRTransformerDecoder(vocab_size, d_model, nhead=4, num_layers=3, max_len=max_seq_len)\n",
        "    def forward(self, images, tgt_input, tgt_mask=None):\n",
        "        feat = self.cnn(images)    # (B, C, H', W')\n",
        "        B, C, H, W = feat.shape\n",
        "        feat = feat.permute(0,2,3,1).contiguous().view(B, H*W, C) # (B, src_len, C)\n",
        "        feat = self.feat2seq(feat)\n",
        "        logits = self.decoder(feat, tgt_input, tgt_mask)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "jDP4uFkRhsrL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "vocab_size = len(dataset.vocab.idx2char)\n",
        "max_seq_len = 128  # hoặc đúng với giá trị bạn đặt ở Dataset\n",
        "\n",
        "model = OCRNet(vocab_size=vocab_size, max_seq_len=max_seq_len).to(device)\n",
        "\n",
        "images = torch.randn(2, 3, 64, 256).to(device)      # Đúng shape ảnh đã resize\n",
        "labels_in = torch.randint(0, vocab_size, (2, max_seq_len)).to(device)\n",
        "\n",
        "out = model(images, labels_in)\n",
        "print(out.shape)  # kỳ vọng (2, max_seq_len, vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X5YKXsvht5L",
        "outputId": "8b5bb1e5-b59a-454c-f76a-d10be7f96be4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 128, 130])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ocr_loss_fn(logits, labels, pad_idx):\n",
        "    \"\"\"\n",
        "    logits: (B, tgt_len, vocab_size)\n",
        "    labels: (B, tgt_len)\n",
        "    \"\"\"\n",
        "    logits = logits.reshape(-1, logits.size(-1))   # sửa view thành reshape\n",
        "    labels = labels.reshape(-1)                    # sửa view thành reshape\n",
        "    loss = nn.CrossEntropyLoss(ignore_index=pad_idx)(logits, labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "QBjCpwvgh_T5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tgt_mask(tgt_input, pad_idx):\n",
        "    # tgt_input: (B, tgt_len)\n",
        "    B, tgt_len = tgt_input.shape\n",
        "    # Mask phía phải (không cho nhìn trước)\n",
        "    tgt_mask = torch.triu(torch.ones((tgt_len, tgt_len)), diagonal=1).bool().to(tgt_input.device)\n",
        "    return tgt_mask  # (tgt_len, tgt_len)"
      ],
      "metadata": {
        "id": "GzBGOSS8h_7e"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, device, pad_idx):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels, _ in dataloader:    # <-- SỬA DÒNG NÀY\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        tgt_input = labels[:, :-1]\n",
        "        tgt_out = labels[:, 1:]\n",
        "        tgt_mask = create_tgt_mask(tgt_input, pad_idx)\n",
        "        logits = model(images, tgt_input, tgt_mask)\n",
        "        loss = ocr_loss_fn(logits, tgt_out, pad_idx)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "1OdPDqSQiBdp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device, pad_idx):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    total_count = 0\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        tgt_input = labels[:, :-1]\n",
        "        tgt_out = labels[:, 1:]\n",
        "        tgt_mask = create_tgt_mask(tgt_input, pad_idx)\n",
        "        logits = model(images, tgt_input, tgt_mask)\n",
        "        loss = ocr_loss_fn(logits, tgt_out, pad_idx)\n",
        "        total_loss += loss.item()\n",
        "        # Đo char-level accuracy\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        mask = (tgt_out != pad_idx)\n",
        "        acc = ((preds == tgt_out) & mask).sum().item() / mask.sum().item()\n",
        "        total_acc += acc\n",
        "        total_count += 1\n",
        "    return total_loss / total_count, total_acc / total_count"
      ],
      "metadata": {
        "id": "OXaxx1oLiM2O"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pad_idx = dataset.vocab.pad_idx\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Tạo DataLoader (giả sử class VinTextOCRDataset đã được import)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "# Nếu có tập validation, bạn cũng tạo val_loader tương tự\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, device, pad_idx)\n",
        "    print(f\"Epoch {epoch+1} | Train loss: {train_loss:.4f}\")\n",
        "    # Nếu có val_loader thì thêm đánh giá:\n",
        "    # val_loss, val_acc = evaluate(model, val_loader, device, pad_idx)\n",
        "    # print(f\"Val loss: {val_loss:.4f} | Val acc: {val_acc:.4f}\")\n",
        "\n",
        "# Lưu model\n",
        "torch.save(model.state_dict(), \"/content/ocr_cnn_transformer.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liucQBFiiNmF",
        "outputId": "e71eac2c-f038-4791-f580-01fa6d119c26"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train loss: 3.3751\n",
            "Epoch 2 | Train loss: 2.7316\n",
            "Epoch 3 | Train loss: 2.5947\n",
            "Epoch 4 | Train loss: 2.5345\n",
            "Epoch 5 | Train loss: 2.4917\n",
            "Epoch 6 | Train loss: 2.4565\n",
            "Epoch 7 | Train loss: 2.4216\n",
            "Epoch 8 | Train loss: 2.3981\n",
            "Epoch 9 | Train loss: 2.3683\n",
            "Epoch 10 | Train loss: 2.3434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "def predict_image(model, image, vocab, device, max_len=48):\n",
        "    model.eval()\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((32, 128)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
        "    tgt_input = torch.tensor([[vocab.bos_idx]], dtype=torch.long).to(device)\n",
        "    decoded = []\n",
        "    for _ in range(max_len):\n",
        "        tgt_mask = torch.triu(torch.ones((tgt_input.shape[1], tgt_input.shape[1]), device=device), diagonal=1).bool()\n",
        "        logits = model(img_tensor, tgt_input, tgt_mask)\n",
        "        next_token = logits[:, -1, :].argmax(-1).item()\n",
        "        print(f\"Step {_}: Token idx {next_token} - char '{vocab.idx2char[next_token]}'\")\n",
        "        if next_token == vocab.eos_idx or next_token == vocab.pad_idx:\n",
        "            break\n",
        "        decoded.append(next_token)\n",
        "        tgt_input = torch.cat([tgt_input, torch.tensor([[next_token]], dtype=torch.long).to(device)], dim=1)\n",
        "    print(\"Full token idx sequence:\", decoded)\n",
        "    return vocab.decode(decoded)\n",
        "\n",
        "# Đường dẫn tới ảnh bạn muốn test\n",
        "img_path = \"/content/vintext/test_image/im1249.jpg\"  # Đổi tên file nếu cần\n",
        "\n",
        "# Đọc ảnh và dự đoán\n",
        "image = Image.open(img_path).convert(\"RGB\")\n",
        "predicted_text = predict_image(model, image, dataset.vocab, device)\n",
        "print(f\"Ảnh: {img_path}\")\n",
        "print(f\"Chuỗi văn bản model sinh ra: {predicted_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wernA2dCwmzN",
        "outputId": "505525a8-42b9-4832-bd54-822a84066e68"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Token idx 56 - char 't'\n",
            "Step 1: Token idx 44 - char 'h'\n",
            "Step 2: Token idx 37 - char 'a'\n",
            "Step 3: Token idx 50 - char 'n'\n",
            "Step 4: Token idx 4 - char ' '\n",
            "Step 5: Token idx 56 - char 't'\n",
            "Step 6: Token idx 44 - char 'h'\n",
            "Step 7: Token idx 4 - char ' '\n",
            "Step 8: Token idx 56 - char 't'\n",
            "Step 9: Token idx 44 - char 'h'\n",
            "Step 10: Token idx 4 - char ' '\n",
            "Step 11: Token idx 56 - char 't'\n",
            "Step 12: Token idx 44 - char 'h'\n",
            "Step 13: Token idx 64 - char 'à'\n",
            "Step 14: Token idx 50 - char 'n'\n",
            "Step 15: Token idx 43 - char 'g'\n",
            "Step 16: Token idx 4 - char ' '\n",
            "Step 17: Token idx 56 - char 't'\n",
            "Step 18: Token idx 44 - char 'h'\n",
            "Step 19: Token idx 4 - char ' '\n",
            "Step 20: Token idx 56 - char 't'\n",
            "Step 21: Token idx 44 - char 'h'\n",
            "Step 22: Token idx 4 - char ' '\n",
            "Step 23: Token idx 56 - char 't'\n",
            "Step 24: Token idx 44 - char 'h'\n",
            "Step 25: Token idx 4 - char ' '\n",
            "Step 26: Token idx 56 - char 't'\n",
            "Step 27: Token idx 44 - char 'h'\n",
            "Step 28: Token idx 64 - char 'à'\n",
            "Step 29: Token idx 50 - char 'n'\n",
            "Step 30: Token idx 44 - char 'h'\n",
            "Step 31: Token idx 4 - char ' '\n",
            "Step 32: Token idx 56 - char 't'\n",
            "Step 33: Token idx 44 - char 'h'\n",
            "Step 34: Token idx 4 - char ' '\n",
            "Step 35: Token idx 56 - char 't'\n",
            "Step 36: Token idx 44 - char 'h'\n",
            "Step 37: Token idx 4 - char ' '\n",
            "Step 38: Token idx 56 - char 't'\n",
            "Step 39: Token idx 44 - char 'h'\n",
            "Step 40: Token idx 4 - char ' '\n",
            "Step 41: Token idx 56 - char 't'\n",
            "Step 42: Token idx 44 - char 'h'\n",
            "Step 43: Token idx 4 - char ' '\n",
            "Step 44: Token idx 56 - char 't'\n",
            "Step 45: Token idx 44 - char 'h'\n",
            "Step 46: Token idx 4 - char ' '\n",
            "Step 47: Token idx 56 - char 't'\n",
            "Full token idx sequence: [56, 44, 37, 50, 4, 56, 44, 4, 56, 44, 4, 56, 44, 64, 50, 43, 4, 56, 44, 4, 56, 44, 4, 56, 44, 4, 56, 44, 64, 50, 44, 4, 56, 44, 4, 56, 44, 4, 56, 44, 4, 56, 44, 4, 56, 44, 4, 56]\n",
            "Ảnh: /content/vintext/test_image/im1249.jpg\n",
            "Chuỗi văn bản model sinh ra: than th th thàng th th th thành th th th th th t\n"
          ]
        }
      ]
    }
  ]
}