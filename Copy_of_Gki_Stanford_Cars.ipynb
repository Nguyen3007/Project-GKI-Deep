{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "WPv-lD6CtJtu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kết nối với Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "project_dir = \"/content/drive/MyDrive/Project_Gki\"\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "print(\"Nội dung trong Project_Gki:\")\n",
        "print(os.listdir(project_dir))\n",
        "\n",
        "# Giải nén dữ liệu nếu có\n",
        "zip_path = os.path.join(project_dir, \"StanfordCars.zip\")\n",
        "print(\"Đường dẫn zip:\", zip_path)\n",
        "print(\"Tồn tại file zip?\", os.path.exists(zip_path))\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"/content/stanford_cars\")\n",
        "    print(\"Đã giải nén StanfordCars.zip\")\n",
        "else:\n",
        "    print(\"File zip không tồn tại!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EeNge8DuFhz",
        "outputId": "13c8f31a-e433-4f20-fe80-9bc285e094b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Nội dung trong Project_Gki:\n",
            "['StanfordCars.zip', 'Copy of Gki_Stanford_Cars.ipynb']\n",
            "Đường dẫn zip: /content/drive/MyDrive/Project_Gki/StanfordCars.zip\n",
            "Tồn tại file zip? True\n",
            "Đã giải nén StanfordCars.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Phần 2: Tạo dữ liệu sử dụng mô hình VQA (BLIP)\n",
        "!pip install transformers pillow torch\n",
        "\n",
        "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
        "\n",
        "\n",
        "# Tải mô hình BLIP\n",
        "print(\"Đang tải mô hình BLIP...\")\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "\n",
        "# Chuyển mô hình sang GPU nếu có\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Sử dụng thiết bị: {device}\")\n",
        "\n",
        "# Định nghĩa danh sách câu hỏi cố định\n",
        "question_templates = [\n",
        "    \"How many cars are in the image?\",\n",
        "    \"What is the color of the car?\",\n",
        "    \"What type of car is this?\",\n",
        "    \"What brand is this car?\",\n",
        "    \"Where is the car located?\"\n",
        "]\n",
        "\n",
        "# Ánh xạ sang tiếng Việt\n",
        "question_translations = {\n",
        "    \"How many cars are in the image?\": \"Có bao nhiêu xe trong ảnh?\",\n",
        "    \"What is the color of the car?\": \"Màu sắc của xe là gì?\",\n",
        "    \"What type of car is this?\": \"Đây là loại xe gì?\",\n",
        "    \"What brand is this car?\": \"Xe này được sản xuất bởi hãng nào?\",\n",
        "    \"Where is the car located?\": \"Xe trong ảnh đang ở đâu?\"\n",
        "}\n",
        "\n",
        "def simple_translate(text):\n",
        "    \"\"\"Hàm dịch đơn giản cho một số câu trả lời phổ biến\"\"\"\n",
        "    translations = {\n",
        "        \"one\": \"một\",\n",
        "        \"two\": \"hai\",\n",
        "        \"three\": \"ba\",\n",
        "        \"four\": \"bốn\",\n",
        "        \"five\": \"năm\",\n",
        "        \"car\": \"xe\",\n",
        "        \"red\": \"đỏ\",\n",
        "        \"blue\": \"xanh dương\",\n",
        "        \"green\": \"xanh lá\",\n",
        "        \"black\": \"đen\",\n",
        "        \"white\": \"trắng\",\n",
        "        \"silver\": \"bạc\",\n",
        "        \"gray\": \"xám\",\n",
        "        \"yellow\": \"vàng\",\n",
        "        \"sedan\": \"sedan\",\n",
        "        \"suv\": \"SUV\",\n",
        "        \"sports car\": \"xe thể thao\",\n",
        "        \"truck\": \"xe tải\",\n",
        "        \"toyota\": \"Toyota\",\n",
        "        \"honda\": \"Honda\",\n",
        "        \"bmw\": \"BMW\",\n",
        "        \"mercedes\": \"Mercedes\",\n",
        "        \"audi\": \"Audi\",\n",
        "        \"ford\": \"Ford\",\n",
        "        \"yes\": \"có\",\n",
        "        \"no\": \"không\",\n",
        "        \"street\": \"đường phố\",\n",
        "        \"parking lot\": \"bãi đỗ xe\",\n",
        "        \"garage\": \"garage\",\n",
        "        \"parked\": \"đỗ\",\n",
        "        \"moving\": \"di chuyển\"\n",
        "    }\n",
        "\n",
        "    # Chuyển text về chữ thường để dễ tìm kiếm\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Thử tìm và thay thế các từ/cụm từ có trong từ điển\n",
        "    for en, vi in translations.items():\n",
        "        if en in text_lower:\n",
        "            text_lower = text_lower.replace(en, vi)\n",
        "\n",
        "    # Tạo câu trả lời hoàn chỉnh\n",
        "    if \"xe\" not in text_lower and \"chiếc\" not in text_lower and any(keyword in text_lower for keyword in [\"một\", \"hai\", \"ba\", \"bốn\", \"năm\"]):\n",
        "        text_lower = text_lower.replace(\"một\", \"một chiếc xe\")\n",
        "        text_lower = text_lower.replace(\"hai\", \"hai chiếc xe\")\n",
        "        text_lower = text_lower.replace(\"ba\", \"ba chiếc xe\")\n",
        "\n",
        "    # Capitalize first letter\n",
        "    if len(text_lower) > 0:\n",
        "        text_lower = text_lower[0].upper() + text_lower[1:]\n",
        "\n",
        "    # Thêm dấu chấm nếu chưa có\n",
        "    if len(text_lower) > 0 and text_lower[-1] not in [\".\", \"!\", \"?\"]:\n",
        "        text_lower += \".\"\n",
        "\n",
        "    return text_lower\n",
        "\n",
        "def generate_qa_with_vqa_model(image_path):\n",
        "    \"\"\"Sử dụng mô hình VQA để trả lời các câu hỏi định sẵn\"\"\"\n",
        "    try:\n",
        "        # Đọc hình ảnh\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        filename = os.path.basename(image_path)\n",
        "\n",
        "        qa_pairs = []\n",
        "\n",
        "        # Đặt câu hỏi và lấy câu trả lời\n",
        "        for question_en in question_templates:\n",
        "            # Xử lý câu hỏi và hình ảnh\n",
        "            inputs = processor(image, question_en, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            # Lấy câu trả lời\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(**inputs)\n",
        "                answer_en = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Thêm vào danh sách kết quả với câu hỏi tiếng Việt\n",
        "            question_vi = question_translations[question_en]\n",
        "\n",
        "            # Dịch câu trả lời sang tiếng Việt\n",
        "            answer_vi = simple_translate(answer_en)\n",
        "\n",
        "            qa_pairs.append({\n",
        "                \"image\": filename,\n",
        "                \"question\": question_vi,\n",
        "                \"answer\": answer_vi\n",
        "            })\n",
        "\n",
        "        return qa_pairs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi khi xử lý ảnh {image_path}: {str(e)}\")\n",
        "        # Trả về câu hỏi-câu trả lời mặc định nếu có lỗi\n",
        "        return [{\"image\": os.path.basename(image_path),\n",
        "                \"question\": \"Có bao nhiêu xe trong ảnh?\",\n",
        "                \"answer\": \"Có một chiếc xe trong ảnh.\"}]\n",
        "\n",
        "def create_dataset_with_vqa(image_dir, output_file, sample_size=None):\n",
        "    \"\"\"Tạo bộ dữ liệu từ thư mục hình ảnh sử dụng mô hình VQA\"\"\"\n",
        "    all_qa_pairs = []\n",
        "\n",
        "    # Kiểm tra thư mục tồn tại\n",
        "    if not os.path.exists(image_dir):\n",
        "        print(f\"Thư mục {image_dir} không tồn tại!\")\n",
        "        return\n",
        "\n",
        "    # Lấy tất cả các file hình ảnh\n",
        "    image_files = [f for f in os.listdir(image_dir)\n",
        "                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    if not image_files:\n",
        "        print(f\"Không tìm thấy hình ảnh nào trong thư mục {image_dir}\")\n",
        "        return\n",
        "\n",
        "    # Nếu có giới hạn số lượng mẫu\n",
        "    if sample_size and sample_size < len(image_files):\n",
        "        image_files = random.sample(image_files, sample_size)\n",
        "\n",
        "    print(f\"Đang tạo bộ dữ liệu từ {len(image_files)} hình ảnh...\")\n",
        "\n",
        "    # Xử lý từng hình ảnh\n",
        "    for i, image_file in enumerate(image_files):\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        print(f\"Đang xử lý ảnh {i+1}/{len(image_files)}: {image_file}\")\n",
        "\n",
        "        # Sử dụng mô hình VQA để sinh câu trả lời\n",
        "        qa_pairs = generate_qa_with_vqa_model(image_path)\n",
        "        all_qa_pairs.extend(qa_pairs)\n",
        "\n",
        "        # Lưu kết quả tạm thời sau mỗi 10 hình ảnh\n",
        "        if (i + 1) % 10 == 0:\n",
        "            with open(output_file + \"_temp.json\", 'w', encoding='utf-8') as f:\n",
        "                json.dump(all_qa_pairs, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Lưu vào file JSON\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_qa_pairs, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Đã tạo {len(all_qa_pairs)} cặp câu hỏi-trả lời trong file {output_file}\")\n",
        "    return all_qa_pairs\n",
        "\n",
        "# Chỉ tạo dữ liệu với 100 ảnh đầu tiên để tiết kiệm thời gian\n",
        "IMAGE_DIR = \"/content/stanford_cars/cars_train/cars_train\"\n",
        "OUTPUT_FILE = \"/content/questions_answers_vqa.json\"\n",
        "qa_pairs = create_dataset_with_vqa(IMAGE_DIR, OUTPUT_FILE, sample_size=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf9DVjl2t07k",
        "outputId": "c40e233f-548f-4bfb-ec70-18bc26c13a56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Đang tải mô hình BLIP...\n",
            "Sử dụng thiết bị: cuda\n",
            "Đang tạo bộ dữ liệu từ 100 hình ảnh...\n",
            "Đang xử lý ảnh 1/100: 04433.jpg\n",
            "Đang xử lý ảnh 2/100: 06033.jpg\n",
            "Đang xử lý ảnh 3/100: 05034.jpg\n",
            "Đang xử lý ảnh 4/100: 06444.jpg\n",
            "Đang xử lý ảnh 5/100: 06470.jpg\n",
            "Đang xử lý ảnh 6/100: 05567.jpg\n",
            "Đang xử lý ảnh 7/100: 04441.jpg\n",
            "Đang xử lý ảnh 8/100: 04777.jpg\n",
            "Đang xử lý ảnh 9/100: 07830.jpg\n",
            "Đang xử lý ảnh 10/100: 02258.jpg\n",
            "Đang xử lý ảnh 11/100: 07937.jpg\n",
            "Đang xử lý ảnh 12/100: 05105.jpg\n",
            "Đang xử lý ảnh 13/100: 05044.jpg\n",
            "Đang xử lý ảnh 14/100: 04167.jpg\n",
            "Đang xử lý ảnh 15/100: 02075.jpg\n",
            "Đang xử lý ảnh 16/100: 05208.jpg\n",
            "Đang xử lý ảnh 17/100: 01731.jpg\n",
            "Đang xử lý ảnh 18/100: 05294.jpg\n",
            "Đang xử lý ảnh 19/100: 07558.jpg\n",
            "Đang xử lý ảnh 20/100: 03237.jpg\n",
            "Đang xử lý ảnh 21/100: 06579.jpg\n",
            "Đang xử lý ảnh 22/100: 01201.jpg\n",
            "Đang xử lý ảnh 23/100: 07674.jpg\n",
            "Đang xử lý ảnh 24/100: 01356.jpg\n",
            "Đang xử lý ảnh 25/100: 00538.jpg\n",
            "Đang xử lý ảnh 26/100: 06793.jpg\n",
            "Đang xử lý ảnh 27/100: 07824.jpg\n",
            "Đang xử lý ảnh 28/100: 03228.jpg\n",
            "Đang xử lý ảnh 29/100: 00535.jpg\n",
            "Đang xử lý ảnh 30/100: 06374.jpg\n",
            "Đang xử lý ảnh 31/100: 07301.jpg\n",
            "Đang xử lý ảnh 32/100: 02705.jpg\n",
            "Đang xử lý ảnh 33/100: 05816.jpg\n",
            "Đang xử lý ảnh 34/100: 03131.jpg\n",
            "Đang xử lý ảnh 35/100: 08096.jpg\n",
            "Đang xử lý ảnh 36/100: 01664.jpg\n",
            "Đang xử lý ảnh 37/100: 07424.jpg\n",
            "Đang xử lý ảnh 38/100: 04639.jpg\n",
            "Đang xử lý ảnh 39/100: 07639.jpg\n",
            "Đang xử lý ảnh 40/100: 03080.jpg\n",
            "Đang xử lý ảnh 41/100: 00985.jpg\n",
            "Đang xử lý ảnh 42/100: 00583.jpg\n",
            "Đang xử lý ảnh 43/100: 06889.jpg\n",
            "Đang xử lý ảnh 44/100: 06990.jpg\n",
            "Đang xử lý ảnh 45/100: 04099.jpg\n",
            "Đang xử lý ảnh 46/100: 00753.jpg\n",
            "Đang xử lý ảnh 47/100: 06403.jpg\n",
            "Đang xử lý ảnh 48/100: 06113.jpg\n",
            "Đang xử lý ảnh 49/100: 03655.jpg\n",
            "Đang xử lý ảnh 50/100: 06539.jpg\n",
            "Đang xử lý ảnh 51/100: 03522.jpg\n",
            "Đang xử lý ảnh 52/100: 01282.jpg\n",
            "Đang xử lý ảnh 53/100: 07863.jpg\n",
            "Đang xử lý ảnh 54/100: 00363.jpg\n",
            "Đang xử lý ảnh 55/100: 01578.jpg\n",
            "Đang xử lý ảnh 56/100: 06301.jpg\n",
            "Đang xử lý ảnh 57/100: 07361.jpg\n",
            "Đang xử lý ảnh 58/100: 04300.jpg\n",
            "Đang xử lý ảnh 59/100: 06183.jpg\n",
            "Đang xử lý ảnh 60/100: 04536.jpg\n",
            "Đang xử lý ảnh 61/100: 02982.jpg\n",
            "Đang xử lý ảnh 62/100: 01202.jpg\n",
            "Đang xử lý ảnh 63/100: 07835.jpg\n",
            "Đang xử lý ảnh 64/100: 07367.jpg\n",
            "Đang xử lý ảnh 65/100: 03341.jpg\n",
            "Đang xử lý ảnh 66/100: 06009.jpg\n",
            "Đang xử lý ảnh 67/100: 02939.jpg\n",
            "Đang xử lý ảnh 68/100: 00160.jpg\n",
            "Đang xử lý ảnh 69/100: 07447.jpg\n",
            "Đang xử lý ảnh 70/100: 06868.jpg\n",
            "Đang xử lý ảnh 71/100: 01494.jpg\n",
            "Đang xử lý ảnh 72/100: 04577.jpg\n",
            "Đang xử lý ảnh 73/100: 05226.jpg\n",
            "Đang xử lý ảnh 74/100: 04717.jpg\n",
            "Đang xử lý ảnh 75/100: 07056.jpg\n",
            "Đang xử lý ảnh 76/100: 02841.jpg\n",
            "Đang xử lý ảnh 77/100: 03306.jpg\n",
            "Đang xử lý ảnh 78/100: 02639.jpg\n",
            "Đang xử lý ảnh 79/100: 00277.jpg\n",
            "Đang xử lý ảnh 80/100: 01373.jpg\n",
            "Đang xử lý ảnh 81/100: 03220.jpg\n",
            "Đang xử lý ảnh 82/100: 00721.jpg\n",
            "Đang xử lý ảnh 83/100: 06716.jpg\n",
            "Đang xử lý ảnh 84/100: 06592.jpg\n",
            "Đang xử lý ảnh 85/100: 05852.jpg\n",
            "Đang xử lý ảnh 86/100: 00235.jpg\n",
            "Đang xử lý ảnh 87/100: 04913.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "# Phần 3: Xây dựng mô hình VQA với CNN và LSTM\n",
        "# Transform ảnh\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def load_image(image_path):\n",
        "    try:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        return transform(image)\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi khi tải hình ảnh {image_path}: {e}\")\n",
        "        # Trả về tensor trống nếu không tải được hình ảnh\n",
        "        return torch.zeros(3, 224, 224)\n",
        "\n",
        "# Xây dựng từ điển\n",
        "def build_vocab(data):\n",
        "    vocab = set()\n",
        "    for item in data:\n",
        "        vocab.update(item['question'].split())\n",
        "        vocab.update(item['answer'].split())\n",
        "    vocab = ['<PAD>', '<UNK>', '<START>', '<END>'] + list(vocab)\n",
        "    return {word: idx for idx, word in enumerate(vocab)}, {idx: word for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Dataset\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, data, word2idx, image_dir):\n",
        "        self.data = data\n",
        "        self.word2idx = word2idx\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image_path = os.path.join(self.image_dir, item['image'])\n",
        "        image = load_image(image_path)\n",
        "\n",
        "        # Chuyển câu hỏi thành token\n",
        "        question = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in item['question'].split()]\n",
        "\n",
        "        # Chuyển câu trả lời thành token\n",
        "        answer = [self.word2idx['<START>']] + [self.word2idx.get(word, self.word2idx['<UNK>'])\n",
        "                                              for word in item['answer'].split()] + [self.word2idx['<END>']]\n",
        "\n",
        "        return image, torch.tensor(question), torch.tensor(answer)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, questions, answers = zip(*batch)\n",
        "    images = torch.stack(images)\n",
        "    questions = nn.utils.rnn.pad_sequence(questions, batch_first=True, padding_value=0)\n",
        "    answers = nn.utils.rnn.pad_sequence(answers, batch_first=True, padding_value=0)\n",
        "    return images, questions, answers\n",
        "\n",
        "def prepare_dataloaders(data_file, image_dir, batch_size=32):\n",
        "    # Đọc dữ liệu\n",
        "    try:\n",
        "        with open(data_file, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File dữ liệu {data_file} không tồn tại!\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    # Xây dựng từ điển\n",
        "    word2idx, idx2word = build_vocab(data)\n",
        "    vocab_size = len(word2idx)\n",
        "\n",
        "    # Chia dữ liệu\n",
        "    train_size = int(0.8 * len(data))\n",
        "    val_size = int(0.1 * len(data))\n",
        "\n",
        "    train_data = data[:train_size]\n",
        "    val_data = data[train_size:train_size + val_size]\n",
        "    test_data = data[train_size + val_size:]\n",
        "\n",
        "    # Tạo datasets\n",
        "    train_dataset = QADataset(train_data, word2idx, image_dir)\n",
        "    val_dataset = QADataset(val_data, word2idx, image_dir)\n",
        "    test_dataset = QADataset(test_data, word2idx, image_dir)\n",
        "\n",
        "    # Tạo dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=1,  # Để dễ xử lý, batch_size cho test_loader là 1\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader, word2idx, idx2word\n",
        "\n",
        "# CNN Encoder (với lựa chọn pretrained hoặc train từ đầu)\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, embed_size=512, pretrained=True):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "        if pretrained:\n",
        "            cnn = models.resnet50(pretrained=True)\n",
        "            # Đóng băng các tham số nếu sử dụng pretrained\n",
        "            for param in cnn.parameters():\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            # Train từ đầu\n",
        "            cnn = models.resnet50(pretrained=False)\n",
        "\n",
        "        self.cnn = nn.Sequential(*list(cnn.children())[:-1])\n",
        "        self.fc = nn.Linear(2048, embed_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.cnn(images)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.fc(features)\n",
        "        return features\n",
        "\n",
        "# Bộ mã hóa câu hỏi\n",
        "class QuestionEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(QuestionEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, questions):\n",
        "        embeddings = self.embedding(questions)\n",
        "        outputs, (hidden, cell) = self.lstm(embeddings)\n",
        "        features = self.fc(hidden.squeeze(0))\n",
        "        return outputs, features\n",
        "\n",
        "# LSTM Decoder với Attention\n",
        "class AttentionLSTMDecoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "        super(AttentionLSTMDecoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.attention = nn.Linear(embed_size + hidden_size, hidden_size)\n",
        "        self.attention_combine = nn.Linear(hidden_size * 2 + embed_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size + hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, img_features, q_features, captions):\n",
        "        batch_size = captions.size(0)\n",
        "        embeddings = self.embedding(captions)\n",
        "\n",
        "        # Kết hợp đặc trưng câu hỏi với đặc trưng hình ảnh\n",
        "        combined_features = torch.cat((img_features, q_features), dim=1)\n",
        "        context = self.attention(combined_features)\n",
        "\n",
        "        # Lặp lại ngữ cảnh cho mỗi từ trong chuỗi\n",
        "        context = context.unsqueeze(1).repeat(1, embeddings.size(1), 1)\n",
        "\n",
        "        # Nối embeddings và ngữ cảnh\n",
        "        lstm_input = torch.cat((embeddings, context), dim=2)\n",
        "\n",
        "        # LSTM và đầu ra\n",
        "        hiddens, _ = self.lstm(lstm_input)\n",
        "        outputs = self.fc(hiddens)\n",
        "        return outputs\n",
        "\n",
        "# LSTM Decoder không có Attention\n",
        "class BasicLSTMDecoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "        super(BasicLSTMDecoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size + embed_size + hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, img_features, q_features, captions):\n",
        "        embeddings = self.embedding(captions)\n",
        "\n",
        "        # Kết hợp đặc trưng\n",
        "        combined_features = torch.cat((img_features, q_features), dim=1)\n",
        "\n",
        "        # Lặp lại cho mỗi từ\n",
        "        features = combined_features.unsqueeze(1).repeat(1, embeddings.size(1), 1)\n",
        "        inputs = torch.cat((features, embeddings), dim=2)\n",
        "\n",
        "        # LSTM và đầu ra\n",
        "        hiddens, _ = self.lstm(inputs)\n",
        "        outputs = self.fc(hiddens)\n",
        "        return outputs\n",
        "\n",
        "# Mô hình VQA hoàn chỉnh\n",
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, use_pretrained=True, use_attention=True):\n",
        "        super(VQAModel, self).__init__()\n",
        "        self.cnn = CNNEncoder(embed_size, pretrained=use_pretrained)\n",
        "        self.question_encoder = QuestionEncoder(vocab_size, embed_size, hidden_size)\n",
        "\n",
        "        if use_attention:\n",
        "            self.decoder = AttentionLSTMDecoder(embed_size, hidden_size, vocab_size)\n",
        "        else:\n",
        "            self.decoder = BasicLSTMDecoder(embed_size, hidden_size, vocab_size)\n",
        "\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "    def forward(self, images, questions, answers=None):\n",
        "        img_features = self.cnn(images)\n",
        "        q_outputs, q_features = self.question_encoder(questions)\n",
        "\n",
        "        if answers is not None:\n",
        "            # Chế độ huấn luyện - teacher forcing\n",
        "            outputs = self.decoder(img_features, q_features, answers[:, :-1])\n",
        "            return outputs\n",
        "        else:\n",
        "            # Chế độ suy luận\n",
        "            return img_features, q_features"
      ],
      "metadata": {
        "id": "5Y-5YP8qu4G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Không dùng tới phần dưới"
      ],
      "metadata": {
        "id": "qr1A4eO2uC__"
      }
    }
  ]
}