{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "project_dir = \"/content/drive/MyDrive/Project_Gki\"\n",
        "print(\"Nội dung trong Project_Gki:\")\n",
        "print(os.listdir(project_dir))\n",
        "\n",
        "zip_path = os.path.join(project_dir, \"StanfordCars.zip\")\n",
        "print(\"Đường dẫn zip:\", zip_path)\n",
        "print(\"Tồn tại file zip?\", os.path.exists(zip_path))\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"/content/stanford_cars\")\n",
        "    print(\"Đã giải nén StanfordCars.zip\")\n",
        "else:\n",
        "    print(\"File zip không tồn tại!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EeNge8DuFhz",
        "outputId": "fb837c0a-6ae8-4d1a-a3fb-f0fa64a30117"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Nội dung trong Project_Gki:\n",
            "['StanfordCars.zip', 'Copy of Gki_Stanford_Cars.ipynb']\n",
            "Đường dẫn zip: /content/drive/MyDrive/Project_Gki/StanfordCars.zip\n",
            "Tồn tại file zip? True\n",
            "Đã giải nén StanfordCars.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Đường dẫn đến thư mục chứa ảnh\n",
        "IMAGE_DIR = \"/content/stanford_cars/cars_train/cars_train\"\n",
        "OUTPUT_FILE = \"/content/questions_answers.json\"\n",
        "\n",
        "# Các loại câu hỏi và câu trả lời mẫu\n",
        "question_templates = [\n",
        "    \"Có bao nhiêu xe trong ảnh?\",\n",
        "    \"Màu sắc của xe là gì?\",\n",
        "    \"Đây là loại xe gì?\",\n",
        "    \"Xe này được sản xuất bởi hãng nào?\",\n",
        "    \"Xe trong ảnh đang ở đâu?\"\n",
        "]\n",
        "\n",
        "color_answers = [\"đỏ\", \"xanh dương\", \"xanh lá\", \"đen\", \"trắng\", \"bạc\", \"vàng\", \"cam\", \"xám\"]\n",
        "car_types = [\"sedan\", \"SUV\", \"xe thể thao\", \"xe bán tải\", \"xe tải\", \"xe sang\"]\n",
        "car_brands = [\"Toyota\", \"Honda\", \"BMW\", \"Mercedes\", \"Ford\", \"Audi\", \"Hyundai\", \"Kia\"]\n",
        "locations = [\"trên đường phố\", \"trong bãi đỗ xe\", \"trước tòa nhà\", \"trong garage\", \"bên đường\"]\n",
        "\n",
        "def generate_qa_pairs(image_path):\n",
        "    \"\"\"Tạo câu hỏi và câu trả lời cho một hình ảnh\"\"\"\n",
        "    filename = os.path.basename(image_path)\n",
        "    qa_pairs = []\n",
        "\n",
        "    # Câu hỏi về số lượng (giả định mỗi ảnh chỉ có một chiếc xe)\n",
        "    qa_pairs.append({\n",
        "        \"image\": filename,\n",
        "        \"question\": \"Có bao nhiêu xe trong ảnh?\",\n",
        "        \"answer\": \"Có một chiếc xe trong ảnh.\"\n",
        "    })\n",
        "\n",
        "    # Câu hỏi về màu sắc\n",
        "    color = random.choice(color_answers)\n",
        "    qa_pairs.append({\n",
        "        \"image\": filename,\n",
        "        \"question\": \"Màu sắc của xe là gì?\",\n",
        "        \"answer\": f\"Chiếc xe có màu {color}.\"\n",
        "    })\n",
        "\n",
        "    # Câu hỏi về loại xe\n",
        "    car_type = random.choice(car_types)\n",
        "    qa_pairs.append({\n",
        "        \"image\": filename,\n",
        "        \"question\": \"Đây là loại xe gì?\",\n",
        "        \"answer\": f\"Đây là một chiếc {car_type}.\"\n",
        "    })\n",
        "\n",
        "    # Câu hỏi về thương hiệu\n",
        "    brand = random.choice(car_brands)\n",
        "    qa_pairs.append({\n",
        "        \"image\": filename,\n",
        "        \"question\": \"Xe này được sản xuất bởi hãng nào?\",\n",
        "        \"answer\": f\"Đây là xe {brand}.\"\n",
        "    })\n",
        "\n",
        "    # Câu hỏi về vị trí\n",
        "    location = random.choice(locations)\n",
        "    qa_pairs.append({\n",
        "        \"image\": filename,\n",
        "        \"question\": \"Xe trong ảnh đang ở đâu?\",\n",
        "        \"answer\": f\"Chiếc xe đang ở {location}.\"\n",
        "    })\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "def create_dataset(image_dir, output_file):\n",
        "    \"\"\"Tạo bộ dữ liệu từ thư mục hình ảnh\"\"\"\n",
        "    all_qa_pairs = []\n",
        "\n",
        "    # Kiểm tra thư mục tồn tại\n",
        "    if not os.path.exists(image_dir):\n",
        "        os.makedirs(image_dir)\n",
        "        print(f\"Đã tạo thư mục {image_dir}. Vui lòng thêm các hình ảnh xe vào thư mục này.\")\n",
        "        return\n",
        "\n",
        "    # Lấy tất cả các file hình ảnh\n",
        "    image_files = [f for f in os.listdir(image_dir)\n",
        "                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    if not image_files:\n",
        "        print(f\"Không tìm thấy hình ảnh nào trong thư mục {image_dir}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Tạo bộ dữ liệu từ {len(image_files)} hình ảnh...\")\n",
        "\n",
        "    # Tạo câu hỏi và câu trả lời cho mỗi hình ảnh\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        qa_pairs = generate_qa_pairs(image_path)\n",
        "        all_qa_pairs.extend(qa_pairs)\n",
        "\n",
        "    # Lưu vào file JSON\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_qa_pairs, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Đã tạo {len(all_qa_pairs)} cặp câu hỏi-trả lời trong file {output_file}\")\n",
        "    return all_qa_pairs\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_dataset(IMAGE_DIR, OUTPUT_FILE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5u_AGQlq76u",
        "outputId": "8ee43131-954d-428b-8108-e6e1170a2989"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tạo bộ dữ liệu từ 8144 hình ảnh...\n",
            "Đã tạo 40720 cặp câu hỏi-trả lời trong file /content/questions_answers.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# CNN Encoder (với lựa chọn pretrained hoặc train từ đầu)\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, embed_size=512, pretrained=True):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "        if pretrained:\n",
        "            cnn = models.resnet50(pretrained=True)\n",
        "            # Đóng băng các tham số nếu sử dụng pretrained\n",
        "            for param in cnn.parameters():\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            # Train từ đầu\n",
        "            cnn = models.resnet50(pretrained=False)\n",
        "\n",
        "        self.cnn = nn.Sequential(*list(cnn.children())[:-1])\n",
        "        self.fc = nn.Linear(2048, embed_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.cnn(images)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.fc(features)\n",
        "        return features\n",
        "\n",
        "# Bộ mã hóa câu hỏi\n",
        "class QuestionEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(QuestionEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, questions):\n",
        "        embeddings = self.embedding(questions)\n",
        "        outputs, (hidden, cell) = self.lstm(embeddings)\n",
        "        features = self.fc(hidden.squeeze(0))\n",
        "        return outputs, features\n",
        "\n",
        "# LSTM Decoder với Attention\n",
        "class AttentionLSTMDecoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "        super(AttentionLSTMDecoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.attention = nn.Linear(embed_size + hidden_size, hidden_size)\n",
        "        self.attention_combine = nn.Linear(hidden_size * 2 + embed_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size + hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, img_features, q_features, captions):\n",
        "        batch_size = captions.size(0)\n",
        "        embeddings = self.embedding(captions)\n",
        "\n",
        "        # Kết hợp đặc trưng câu hỏi với đặc trưng hình ảnh\n",
        "        combined_features = torch.cat((img_features, q_features), dim=1)\n",
        "        context = self.attention(combined_features)\n",
        "\n",
        "        # Lặp lại ngữ cảnh cho mỗi từ trong chuỗi\n",
        "        context = context.unsqueeze(1).repeat(1, embeddings.size(1), 1)\n",
        "\n",
        "        # Nối embeddings và ngữ cảnh\n",
        "        lstm_input = torch.cat((embeddings, context), dim=2)\n",
        "\n",
        "        # LSTM và đầu ra\n",
        "        hiddens, _ = self.lstm(lstm_input)\n",
        "        outputs = self.fc(hiddens)\n",
        "        return outputs\n",
        "\n",
        "# LSTM Decoder không có Attention\n",
        "class BasicLSTMDecoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "        super(BasicLSTMDecoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size + embed_size + hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, img_features, q_features, captions):\n",
        "        embeddings = self.embedding(captions)\n",
        "\n",
        "        # Kết hợp đặc trưng\n",
        "        combined_features = torch.cat((img_features, q_features), dim=1)\n",
        "\n",
        "        # Lặp lại cho mỗi từ\n",
        "        features = combined_features.unsqueeze(1).repeat(1, embeddings.size(1), 1)\n",
        "        inputs = torch.cat((features, embeddings), dim=2)\n",
        "\n",
        "        # LSTM và đầu ra\n",
        "        hiddens, _ = self.lstm(inputs)\n",
        "        outputs = self.fc(hiddens)\n",
        "        return outputs\n",
        "\n",
        "# Mô hình VQA hoàn chỉnh\n",
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, use_pretrained=True, use_attention=True):\n",
        "        super(VQAModel, self).__init__()\n",
        "        self.cnn = CNNEncoder(embed_size, pretrained=use_pretrained)\n",
        "        self.question_encoder = QuestionEncoder(vocab_size, embed_size, hidden_size)\n",
        "\n",
        "        if use_attention:\n",
        "            self.decoder = AttentionLSTMDecoder(embed_size, hidden_size, vocab_size)\n",
        "        else:\n",
        "            self.decoder = BasicLSTMDecoder(embed_size, hidden_size, vocab_size)\n",
        "\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "    def forward(self, images, questions, answers=None):\n",
        "        img_features = self.cnn(images)\n",
        "        q_outputs, q_features = self.question_encoder(questions)\n",
        "\n",
        "        if answers is not None:\n",
        "            # Chế độ huấn luyện - teacher forcing\n",
        "            outputs = self.decoder(img_features, q_features, answers[:, :-1])\n",
        "            return outputs\n",
        "        else:\n",
        "            # Chế độ suy luận\n",
        "            return img_features, q_features"
      ],
      "metadata": {
        "id": "emgqfqgou55g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# Transform ảnh\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def load_image(image_path):\n",
        "    try:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        return transform(image)\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi khi tải hình ảnh {image_path}: {e}\")\n",
        "        # Trả về tensor trống nếu không tải được hình ảnh\n",
        "        return torch.zeros(3, 224, 224)\n",
        "\n",
        "# Xây dựng từ điển\n",
        "def build_vocab(data):\n",
        "    vocab = set()\n",
        "    for item in data:\n",
        "        vocab.update(item['question'].split())\n",
        "        vocab.update(item['answer'].split())\n",
        "    vocab = ['<PAD>', '<UNK>', '<START>', '<END>'] + list(vocab)\n",
        "    return {word: idx for idx, word in enumerate(vocab)}, {idx: word for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Dataset\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, data, word2idx, image_dir):\n",
        "        self.data = data\n",
        "        self.word2idx = word2idx\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image_path = os.path.join(self.image_dir, item['image'])\n",
        "        image = load_image(image_path)\n",
        "\n",
        "        # Chuyển câu hỏi thành token\n",
        "        question = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in item['question'].split()]\n",
        "\n",
        "        # Chuyển câu trả lời thành token\n",
        "        answer = [self.word2idx['<START>']] + [self.word2idx.get(word, self.word2idx['<UNK>'])\n",
        "                                              for word in item['answer'].split()] + [self.word2idx['<END>']]\n",
        "\n",
        "        return image, torch.tensor(question), torch.tensor(answer)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, questions, answers = zip(*batch)\n",
        "    images = torch.stack(images)\n",
        "    questions = nn.utils.rnn.pad_sequence(questions, batch_first=True, padding_value=0)\n",
        "    answers = nn.utils.rnn.pad_sequence(answers, batch_first=True, padding_value=0)\n",
        "    return images, questions, answers\n",
        "\n",
        "def prepare_dataloaders(data_file, image_dir, batch_size=32):\n",
        "    # Đọc dữ liệu\n",
        "    try:\n",
        "        with open(data_file, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File dữ liệu {data_file} không tồn tại!\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    # Xây dựng từ điển\n",
        "    word2idx, idx2word = build_vocab(data)\n",
        "    vocab_size = len(word2idx)\n",
        "\n",
        "    # Chia dữ liệu\n",
        "    train_size = int(0.8 * len(data))\n",
        "    val_size = int(0.1 * len(data))\n",
        "\n",
        "    train_data = data[:train_size]\n",
        "    val_data = data[train_size:train_size + val_size]\n",
        "    test_data = data[train_size + val_size:]\n",
        "\n",
        "    # Tạo datasets\n",
        "    train_dataset = QADataset(train_data, word2idx, image_dir)\n",
        "    val_dataset = QADataset(val_data, word2idx, image_dir)\n",
        "    test_dataset = QADataset(test_data, word2idx, image_dir)\n",
        "\n",
        "    # Tạo dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=1,  # Để dễ xử lý, batch_size cho test_loader là 1\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader, word2idx, idx2word"
      ],
      "metadata": {
        "id": "5-lfM1m6JI0j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cập nhật cell huấn luyện mô hình (thay thế dòng import từ module)\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Thay vì import từ file riêng biệt, ta sẽ sử dụng các class đã định nghĩa trong notebook\n",
        "# from model import VQAModel\n",
        "# from dataset import prepare_dataloaders, load_image\n",
        "\n",
        "# Cấu hình\n",
        "CONFIG = {\n",
        "    'embed_size': 256,\n",
        "    'hidden_size': 512,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 10,\n",
        "    'learning_rate': 0.001,\n",
        "    'image_dir': '/content/stanford_cars/cars_train/cars_train',\n",
        "    'data_file': '/content/questions_answers.json',\n",
        "    'save_dir': '/content/models',\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "}\n",
        "\n",
        "# Tạo thư mục lưu mô hình\n",
        "if not os.path.exists(CONFIG['save_dir']):\n",
        "    os.makedirs(CONFIG['save_dir'])\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Huấn luyện mô hình qua 1 epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, (images, questions, answers) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        questions = questions.to(device)\n",
        "        answers = answers.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, questions, answers)\n",
        "\n",
        "        # Tính loss (bỏ qua padding)\n",
        "        loss = criterion(outputs.reshape(-1, outputs.size(-1)), answers[:, 1:].reshape(-1))\n",
        "\n",
        "        # Backward pass và optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # Gradient clipping\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # In tiến trình\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}, \"\n",
        "                  f\"Time: {time.time() - start_time:.2f}s\")\n",
        "            start_time = time.time()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Đánh giá mô hình trên tập validation\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, questions, answers in val_loader:\n",
        "            images = images.to(device)\n",
        "            questions = questions.to(device)\n",
        "            answers = answers.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images, questions, answers)\n",
        "\n",
        "            # Tính loss\n",
        "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), answers[:, 1:].reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "def generate_answer(model, image, question, word2idx, idx2word, device, max_len=20):\n",
        "    \"\"\"Sinh câu trả lời từ mô hình\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Chuẩn bị đầu vào\n",
        "        image = image.unsqueeze(0).to(device)\n",
        "        question = torch.tensor([word2idx.get(word, word2idx['<UNK>'])\n",
        "                                for word in question.split()]).unsqueeze(0).to(device)\n",
        "\n",
        "        # Lấy đặc trưng\n",
        "        img_features, q_features = model(image, question)\n",
        "\n",
        "        # Sinh câu trả lời\n",
        "        answer = [word2idx['<START>']]\n",
        "        for i in range(max_len):\n",
        "            # Chuyển đổi chuỗi hiện tại thành tensor\n",
        "            current_answer = torch.tensor([answer]).to(device)\n",
        "\n",
        "            # Dự đoán từ tiếp theo\n",
        "            output = model.decoder(img_features, q_features, current_answer)\n",
        "\n",
        "            # Lấy từ có khả năng cao nhất tiếp theo\n",
        "            _, next_word_idx = output[:, -1].max(1)\n",
        "            next_word_idx = next_word_idx.item()\n",
        "\n",
        "            # Thêm vào chuỗi câu trả lời\n",
        "            answer.append(next_word_idx)\n",
        "\n",
        "            # Dừng nếu gặp token <END>\n",
        "            if next_word_idx == word2idx['<END>']:\n",
        "                break\n",
        "\n",
        "        # Chuyển đổi chỉ số thành từ (bỏ qua <START> và <END>)\n",
        "        words = [idx2word[idx] for idx in answer[1:-1] if idx in idx2word]\n",
        "        return ' '.join(words)\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"Huấn luyện các biến thể của mô hình và so sánh hiệu suất\"\"\"\n",
        "    print(f\"Sử dụng thiết bị: {CONFIG['device']}\")\n",
        "\n",
        "    # Chuẩn bị dữ liệu\n",
        "    train_loader, val_loader, test_loader, word2idx, idx2word = prepare_dataloaders(\n",
        "        CONFIG['data_file'],\n",
        "        CONFIG['image_dir'],\n",
        "        CONFIG['batch_size']\n",
        "    )\n",
        "\n",
        "    if train_loader is None:\n",
        "        print(\"Không thể tải dữ liệu. Vui lòng chạy data_generation.py trước.\")\n",
        "        return\n",
        "\n",
        "    vocab_size = len(word2idx)\n",
        "    print(f\"Kích thước từ điển: {vocab_size}\")\n",
        "\n",
        "    # Định nghĩa 4 mô hình khác nhau để so sánh\n",
        "    models = {\n",
        "        \"Pretrained_CNN_with_Attention\": VQAModel(\n",
        "            embed_size=CONFIG['embed_size'],\n",
        "            hidden_size=CONFIG['hidden_size'],\n",
        "            vocab_size=vocab_size,\n",
        "            use_pretrained=True,\n",
        "            use_attention=True\n",
        "        ),\n",
        "        \"Pretrained_CNN_without_Attention\": VQAModel(\n",
        "            embed_size=CONFIG['embed_size'],\n",
        "            hidden_size=CONFIG['hidden_size'],\n",
        "            vocab_size=vocab_size,\n",
        "            use_pretrained=True,\n",
        "            use_attention=False\n",
        "        ),\n",
        "        \"From_Scratch_CNN_with_Attention\": VQAModel(\n",
        "            embed_size=CONFIG['embed_size'],\n",
        "            hidden_size=CONFIG['hidden_size'],\n",
        "            vocab_size=vocab_size,\n",
        "            use_pretrained=False,\n",
        "            use_attention=True\n",
        "        ),\n",
        "        \"From_Scratch_CNN_without_Attention\": VQAModel(\n",
        "            embed_size=CONFIG['embed_size'],\n",
        "            hidden_size=CONFIG['hidden_size'],\n",
        "            vocab_size=vocab_size,\n",
        "            use_pretrained=False,\n",
        "            use_attention=False\n",
        "        )\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Huấn luyện từng mô hình\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Đang huấn luyện {model_name}...\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        model = model.to(CONFIG['device'])\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
        "        optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(CONFIG['epochs']):\n",
        "            print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            # Huấn luyện\n",
        "            train_loss = train_epoch(model, train_loader, criterion, optimizer, CONFIG['device'])\n",
        "            train_losses.append(train_loss)\n",
        "\n",
        "            # Kiểm tra\n",
        "            val_loss = validate(model, val_loader, criterion, CONFIG['device'])\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "            # Điều chỉnh learning rate\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Lưu kết quả\n",
        "        results[model_name] = {\n",
        "            'model': model,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses\n",
        "        }\n",
        "\n",
        "        # Lưu mô hình\n",
        "        torch.save(model.state_dict(), os.path.join(CONFIG['save_dir'], f\"{model_name}.pth\"))\n",
        "        print(f\"Đã lưu mô hình tại {os.path.join(CONFIG['save_dir'], model_name)}.pth\")\n",
        "\n",
        "    # Vẽ đồ thị so sánh\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for model_name, result in results.items():\n",
        "        plt.plot(result['val_losses'], label=f\"{model_name}\")\n",
        "\n",
        "    plt.title('So sánh Loss đánh giá')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(CONFIG['save_dir'], 'model_comparison.png'))\n",
        "    print(f\"Đã lưu biểu đồ so sánh tại {os.path.join(CONFIG['save_dir'], 'model_comparison.png')}\")\n",
        "\n",
        "    # Đánh giá trên tập kiểm tra\n",
        "    print(\"\\nĐánh giá mô hình trên tập kiểm tra...\")\n",
        "\n",
        "    for model_name, result in results.items():\n",
        "        model = result['model']\n",
        "        model.eval()\n",
        "\n",
        "        print(f\"\\nMô hình: {model_name}\")\n",
        "        for i, (image, question, answer) in enumerate(test_loader):\n",
        "            if i >= 3:  # Chỉ hiển thị 3 ví dụ đầu tiên\n",
        "                break\n",
        "\n",
        "            image = image[0]\n",
        "            question_text = ' '.join([idx2word[idx.item()] for idx in question[0] if idx.item() in idx2word])\n",
        "            answer_text = ' '.join([idx2word[idx.item()] for idx in answer[0][1:-1] if idx.item() in idx2word])\n",
        "\n",
        "            generated = generate_answer(model, image, question_text, word2idx, idx2word, CONFIG['device'])\n",
        "\n",
        "            print(f\"Câu hỏi: {question_text}\")\n",
        "            print(f\"Câu trả lời thực: {answer_text}\")\n",
        "            print(f\"Câu trả lời sinh: {generated}\\n\")\n",
        "\n",
        "train_model()  # Gọi trực tiếp hàm train_model trong cell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "yMavjmbDJQY6",
        "outputId": "bf4934ba-460e-433a-e966-c644b01d28ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sử dụng thiết bị: cuda\n",
            "Kích thước từ điển: 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Đang huấn luyện Pretrained_CNN_with_Attention...\n",
            "==================================================\n",
            "\n",
            "Epoch 1/10\n",
            "------------------------------\n",
            "Batch 10/1018, Loss: 1.8178, Time: 3.54s\n",
            "Batch 20/1018, Loss: 0.7905, Time: 3.15s\n",
            "Batch 30/1018, Loss: 0.3574, Time: 3.53s\n",
            "Batch 40/1018, Loss: 0.3067, Time: 3.80s\n",
            "Batch 50/1018, Loss: 0.3444, Time: 3.09s\n",
            "Batch 60/1018, Loss: 0.2473, Time: 3.55s\n",
            "Batch 70/1018, Loss: 0.2716, Time: 3.73s\n",
            "Batch 80/1018, Loss: 0.2310, Time: 3.79s\n",
            "Batch 90/1018, Loss: 0.2545, Time: 3.21s\n",
            "Batch 100/1018, Loss: 0.2788, Time: 3.63s\n",
            "Batch 110/1018, Loss: 0.2525, Time: 4.29s\n",
            "Batch 120/1018, Loss: 0.2757, Time: 3.74s\n",
            "Batch 130/1018, Loss: 0.3202, Time: 4.08s\n",
            "Batch 140/1018, Loss: 0.2541, Time: 3.90s\n",
            "Batch 150/1018, Loss: 0.2176, Time: 4.20s\n",
            "Batch 160/1018, Loss: 0.2376, Time: 3.23s\n",
            "Batch 170/1018, Loss: 0.2977, Time: 3.16s\n",
            "Batch 180/1018, Loss: 0.2299, Time: 4.16s\n",
            "Batch 190/1018, Loss: 0.2605, Time: 3.79s\n",
            "Batch 200/1018, Loss: 0.2214, Time: 3.32s\n",
            "Batch 210/1018, Loss: 0.2737, Time: 3.21s\n",
            "Batch 220/1018, Loss: 0.2632, Time: 3.79s\n",
            "Batch 230/1018, Loss: 0.2436, Time: 3.15s\n",
            "Batch 240/1018, Loss: 0.2518, Time: 3.28s\n",
            "Batch 250/1018, Loss: 0.2612, Time: 3.79s\n",
            "Batch 260/1018, Loss: 0.2604, Time: 3.68s\n",
            "Batch 270/1018, Loss: 0.2468, Time: 3.43s\n",
            "Batch 280/1018, Loss: 0.2751, Time: 3.10s\n",
            "Batch 290/1018, Loss: 0.1978, Time: 3.75s\n",
            "Batch 300/1018, Loss: 0.2202, Time: 3.81s\n",
            "Batch 310/1018, Loss: 0.2153, Time: 2.94s\n",
            "Batch 320/1018, Loss: 0.2309, Time: 3.31s\n",
            "Batch 330/1018, Loss: 0.2838, Time: 4.27s\n",
            "Batch 340/1018, Loss: 0.2631, Time: 3.44s\n",
            "Batch 350/1018, Loss: 0.2217, Time: 3.20s\n",
            "Batch 360/1018, Loss: 0.2085, Time: 3.22s\n",
            "Batch 370/1018, Loss: 0.2034, Time: 4.03s\n",
            "Batch 380/1018, Loss: 0.2990, Time: 3.22s\n",
            "Batch 390/1018, Loss: 0.2240, Time: 2.99s\n",
            "Batch 400/1018, Loss: 0.2702, Time: 3.81s\n",
            "Batch 410/1018, Loss: 0.2682, Time: 3.80s\n",
            "Batch 420/1018, Loss: 0.2157, Time: 3.24s\n",
            "Batch 430/1018, Loss: 0.2411, Time: 3.85s\n",
            "Batch 440/1018, Loss: 0.2301, Time: 4.12s\n",
            "Batch 450/1018, Loss: 0.2180, Time: 3.25s\n",
            "Batch 460/1018, Loss: 0.2103, Time: 3.50s\n",
            "Batch 470/1018, Loss: 0.2458, Time: 3.58s\n",
            "Batch 480/1018, Loss: 0.2480, Time: 4.29s\n",
            "Batch 490/1018, Loss: 0.2645, Time: 3.34s\n",
            "Batch 500/1018, Loss: 0.2597, Time: 3.03s\n",
            "Batch 510/1018, Loss: 0.2329, Time: 3.87s\n",
            "Batch 520/1018, Loss: 0.2515, Time: 3.65s\n",
            "Batch 530/1018, Loss: 0.1978, Time: 3.66s\n",
            "Batch 540/1018, Loss: 0.2398, Time: 3.15s\n",
            "Batch 550/1018, Loss: 0.1727, Time: 4.44s\n",
            "Batch 560/1018, Loss: 0.2520, Time: 3.92s\n",
            "Batch 570/1018, Loss: 0.2705, Time: 3.21s\n",
            "Batch 580/1018, Loss: 0.2684, Time: 3.64s\n",
            "Batch 590/1018, Loss: 0.2943, Time: 3.68s\n",
            "Batch 600/1018, Loss: 0.2107, Time: 2.99s\n",
            "Batch 610/1018, Loss: 0.2488, Time: 3.04s\n",
            "Batch 620/1018, Loss: 0.2873, Time: 3.69s\n",
            "Batch 630/1018, Loss: 0.2360, Time: 3.67s\n",
            "Batch 640/1018, Loss: 0.2005, Time: 3.45s\n",
            "Batch 650/1018, Loss: 0.2327, Time: 3.27s\n",
            "Batch 660/1018, Loss: 0.2724, Time: 4.11s\n",
            "Batch 670/1018, Loss: 0.2105, Time: 3.41s\n",
            "Batch 680/1018, Loss: 0.2835, Time: 3.09s\n",
            "Batch 690/1018, Loss: 0.2387, Time: 3.23s\n",
            "Batch 700/1018, Loss: 0.2396, Time: 5.02s\n",
            "Batch 710/1018, Loss: 0.2161, Time: 3.27s\n",
            "Batch 720/1018, Loss: 0.2568, Time: 3.15s\n",
            "Batch 730/1018, Loss: 0.2105, Time: 3.61s\n",
            "Batch 740/1018, Loss: 0.2488, Time: 3.89s\n",
            "Batch 750/1018, Loss: 0.2321, Time: 3.28s\n",
            "Batch 760/1018, Loss: 0.2240, Time: 2.96s\n",
            "Batch 770/1018, Loss: 0.2294, Time: 3.27s\n",
            "Batch 780/1018, Loss: 0.2548, Time: 3.77s\n",
            "Batch 790/1018, Loss: 0.2598, Time: 2.96s\n",
            "Batch 800/1018, Loss: 0.2634, Time: 3.32s\n",
            "Batch 810/1018, Loss: 0.2681, Time: 3.55s\n",
            "Batch 820/1018, Loss: 0.2572, Time: 4.11s\n",
            "Batch 830/1018, Loss: 0.2631, Time: 4.19s\n",
            "Batch 840/1018, Loss: 0.2027, Time: 3.19s\n",
            "Batch 850/1018, Loss: 0.2351, Time: 3.58s\n",
            "Batch 860/1018, Loss: 0.2461, Time: 3.87s\n",
            "Batch 870/1018, Loss: 0.2497, Time: 3.36s\n",
            "Batch 880/1018, Loss: 0.2350, Time: 3.14s\n",
            "Batch 890/1018, Loss: 0.2771, Time: 4.03s\n",
            "Batch 900/1018, Loss: 0.2204, Time: 3.23s\n",
            "Batch 910/1018, Loss: 0.2762, Time: 3.18s\n",
            "Batch 920/1018, Loss: 0.2608, Time: 3.27s\n",
            "Batch 930/1018, Loss: 0.2322, Time: 3.95s\n",
            "Batch 940/1018, Loss: 0.2398, Time: 3.09s\n",
            "Batch 950/1018, Loss: 0.2253, Time: 3.16s\n",
            "Batch 960/1018, Loss: 0.2818, Time: 3.03s\n",
            "Batch 970/1018, Loss: 0.2702, Time: 4.04s\n",
            "Batch 980/1018, Loss: 0.2367, Time: 3.01s\n",
            "Batch 990/1018, Loss: 0.2304, Time: 2.94s\n",
            "Batch 1000/1018, Loss: 0.2042, Time: 3.25s\n",
            "Batch 1010/1018, Loss: 0.2211, Time: 4.08s\n",
            "Train Loss: 0.2849, Val Loss: 0.2382\n",
            "\n",
            "Epoch 2/10\n",
            "------------------------------\n",
            "Batch 10/1018, Loss: 0.2419, Time: 3.16s\n",
            "Batch 20/1018, Loss: 0.1619, Time: 4.27s\n",
            "Batch 30/1018, Loss: 0.2872, Time: 3.25s\n",
            "Batch 40/1018, Loss: 0.2557, Time: 3.24s\n",
            "Batch 50/1018, Loss: 0.2281, Time: 3.29s\n",
            "Batch 60/1018, Loss: 0.2673, Time: 3.49s\n",
            "Batch 70/1018, Loss: 0.2399, Time: 3.31s\n",
            "Batch 80/1018, Loss: 0.1913, Time: 3.24s\n",
            "Batch 90/1018, Loss: 0.2189, Time: 3.95s\n",
            "Batch 100/1018, Loss: 0.2251, Time: 3.88s\n",
            "Batch 110/1018, Loss: 0.2340, Time: 3.40s\n",
            "Batch 120/1018, Loss: 0.2639, Time: 3.19s\n",
            "Batch 130/1018, Loss: 0.2198, Time: 4.75s\n",
            "Batch 140/1018, Loss: 0.2661, Time: 4.11s\n",
            "Batch 150/1018, Loss: 0.2317, Time: 3.11s\n",
            "Batch 160/1018, Loss: 0.2726, Time: 3.77s\n",
            "Batch 170/1018, Loss: 0.2239, Time: 3.47s\n",
            "Batch 180/1018, Loss: 0.2701, Time: 3.12s\n",
            "Batch 190/1018, Loss: 0.2407, Time: 3.11s\n",
            "Batch 200/1018, Loss: 0.2111, Time: 4.20s\n",
            "Batch 210/1018, Loss: 0.2540, Time: 3.52s\n",
            "Batch 220/1018, Loss: 0.2364, Time: 3.11s\n",
            "Batch 230/1018, Loss: 0.2807, Time: 3.02s\n",
            "Batch 240/1018, Loss: 0.1993, Time: 3.96s\n",
            "Batch 250/1018, Loss: 0.2313, Time: 3.31s\n",
            "Batch 260/1018, Loss: 0.2261, Time: 3.15s\n",
            "Batch 270/1018, Loss: 0.2399, Time: 3.30s\n",
            "Batch 280/1018, Loss: 0.2592, Time: 4.20s\n",
            "Batch 290/1018, Loss: 0.1973, Time: 3.20s\n",
            "Batch 300/1018, Loss: 0.2522, Time: 3.37s\n",
            "Batch 310/1018, Loss: 0.2207, Time: 4.23s\n",
            "Batch 320/1018, Loss: 0.2416, Time: 3.92s\n",
            "Batch 330/1018, Loss: 0.2706, Time: 3.52s\n",
            "Batch 340/1018, Loss: 0.2685, Time: 3.74s\n",
            "Batch 350/1018, Loss: 0.2433, Time: 4.10s\n",
            "Batch 360/1018, Loss: 0.2112, Time: 3.32s\n",
            "Batch 370/1018, Loss: 0.2323, Time: 3.56s\n",
            "Batch 380/1018, Loss: 0.2171, Time: 4.13s\n",
            "Batch 390/1018, Loss: 0.2478, Time: 3.32s\n",
            "Batch 400/1018, Loss: 0.2461, Time: 3.20s\n",
            "Batch 410/1018, Loss: 0.2581, Time: 3.59s\n",
            "Batch 420/1018, Loss: 0.2606, Time: 4.10s\n",
            "Batch 430/1018, Loss: 0.2157, Time: 3.52s\n",
            "Batch 440/1018, Loss: 0.2222, Time: 3.12s\n",
            "Batch 450/1018, Loss: 0.2194, Time: 3.08s\n",
            "Batch 460/1018, Loss: 0.2784, Time: 4.34s\n",
            "Batch 470/1018, Loss: 0.2637, Time: 3.28s\n",
            "Batch 480/1018, Loss: 0.2585, Time: 3.51s\n",
            "Batch 490/1018, Loss: 0.2636, Time: 4.07s\n",
            "Batch 500/1018, Loss: 0.1972, Time: 3.53s\n",
            "Batch 510/1018, Loss: 0.2170, Time: 3.28s\n",
            "Batch 520/1018, Loss: 0.2581, Time: 3.19s\n",
            "Batch 530/1018, Loss: 0.1972, Time: 4.02s\n",
            "Batch 540/1018, Loss: 0.2376, Time: 3.38s\n",
            "Batch 550/1018, Loss: 0.2481, Time: 2.98s\n",
            "Batch 560/1018, Loss: 0.2102, Time: 3.33s\n",
            "Batch 570/1018, Loss: 0.2214, Time: 4.26s\n",
            "Batch 580/1018, Loss: 0.2412, Time: 3.06s\n",
            "Batch 590/1018, Loss: 0.2188, Time: 3.13s\n",
            "Batch 600/1018, Loss: 0.2784, Time: 4.27s\n",
            "Batch 610/1018, Loss: 0.1705, Time: 4.60s\n",
            "Batch 620/1018, Loss: 0.2551, Time: 3.41s\n",
            "Batch 630/1018, Loss: 0.2415, Time: 3.39s\n",
            "Batch 640/1018, Loss: 0.2308, Time: 4.16s\n",
            "Batch 650/1018, Loss: 0.2614, Time: 2.93s\n",
            "Batch 660/1018, Loss: 0.2454, Time: 3.03s\n",
            "Batch 670/1018, Loss: 0.2497, Time: 3.40s\n",
            "Batch 680/1018, Loss: 0.2030, Time: 4.40s\n",
            "Batch 690/1018, Loss: 0.2189, Time: 2.93s\n",
            "Batch 700/1018, Loss: 0.2841, Time: 3.23s\n",
            "Batch 710/1018, Loss: 0.2586, Time: 4.09s\n",
            "Batch 720/1018, Loss: 0.2657, Time: 4.36s\n",
            "Batch 730/1018, Loss: 0.2498, Time: 3.43s\n",
            "Batch 740/1018, Loss: 0.2390, Time: 3.44s\n",
            "Batch 750/1018, Loss: 0.2266, Time: 3.27s\n",
            "Batch 760/1018, Loss: 0.2461, Time: 3.73s\n",
            "Batch 770/1018, Loss: 0.2658, Time: 3.15s\n",
            "Batch 780/1018, Loss: 0.2752, Time: 3.38s\n",
            "Batch 790/1018, Loss: 0.2071, Time: 3.86s\n",
            "Batch 800/1018, Loss: 0.2272, Time: 4.16s\n",
            "Batch 810/1018, Loss: 0.2682, Time: 3.20s\n",
            "Batch 820/1018, Loss: 0.2250, Time: 3.36s\n",
            "Batch 830/1018, Loss: 0.2234, Time: 4.10s\n",
            "Batch 840/1018, Loss: 0.2099, Time: 3.19s\n",
            "Batch 850/1018, Loss: 0.1989, Time: 3.71s\n",
            "Batch 860/1018, Loss: 0.2010, Time: 3.24s\n",
            "Batch 870/1018, Loss: 0.5710, Time: 3.93s\n",
            "Batch 880/1018, Loss: 1.1015, Time: 3.29s\n",
            "Batch 890/1018, Loss: 0.3090, Time: 3.08s\n",
            "Batch 900/1018, Loss: 0.3097, Time: 3.84s\n",
            "Batch 910/1018, Loss: 0.2443, Time: 3.83s\n",
            "Batch 920/1018, Loss: 0.2683, Time: 3.31s\n",
            "Batch 930/1018, Loss: 0.2709, Time: 3.81s\n",
            "Batch 940/1018, Loss: 0.2075, Time: 4.41s\n",
            "Batch 950/1018, Loss: 0.2591, Time: 3.05s\n",
            "Batch 960/1018, Loss: 0.2331, Time: 3.42s\n",
            "Batch 970/1018, Loss: 0.2674, Time: 3.79s\n",
            "Batch 980/1018, Loss: 0.1985, Time: 4.16s\n",
            "Batch 990/1018, Loss: 0.2613, Time: 3.01s\n",
            "Batch 1000/1018, Loss: 0.2471, Time: 3.28s\n",
            "Batch 1010/1018, Loss: 0.2413, Time: 3.68s\n",
            "Train Loss: 0.2546, Val Loss: 0.2398\n",
            "\n",
            "Epoch 3/10\n",
            "------------------------------\n",
            "Batch 10/1018, Loss: 0.2361, Time: 3.24s\n",
            "Batch 20/1018, Loss: 0.2425, Time: 3.52s\n",
            "Batch 30/1018, Loss: 0.1915, Time: 3.93s\n",
            "Batch 40/1018, Loss: 0.2602, Time: 3.38s\n",
            "Batch 50/1018, Loss: 0.2422, Time: 3.24s\n",
            "Batch 60/1018, Loss: 0.2508, Time: 4.07s\n",
            "Batch 70/1018, Loss: 0.2351, Time: 3.14s\n",
            "Batch 80/1018, Loss: 0.2294, Time: 3.27s\n",
            "Batch 90/1018, Loss: 0.2906, Time: 3.30s\n",
            "Batch 100/1018, Loss: 0.2062, Time: 4.24s\n",
            "Batch 110/1018, Loss: 0.2945, Time: 3.25s\n",
            "Batch 120/1018, Loss: 0.2349, Time: 3.86s\n",
            "Batch 130/1018, Loss: 0.2269, Time: 3.90s\n",
            "Batch 140/1018, Loss: 0.2007, Time: 4.39s\n",
            "Batch 150/1018, Loss: 0.2421, Time: 3.71s\n",
            "Batch 160/1018, Loss: 0.2735, Time: 3.33s\n",
            "Batch 170/1018, Loss: 0.2628, Time: 3.63s\n",
            "Batch 180/1018, Loss: 0.2799, Time: 3.88s\n",
            "Batch 190/1018, Loss: 0.3076, Time: 3.25s\n",
            "Batch 200/1018, Loss: 0.2370, Time: 3.20s\n",
            "Batch 210/1018, Loss: 0.1850, Time: 3.81s\n",
            "Batch 220/1018, Loss: 0.2235, Time: 3.61s\n",
            "Batch 230/1018, Loss: 0.2185, Time: 3.21s\n",
            "Batch 240/1018, Loss: 0.2089, Time: 4.05s\n",
            "Batch 250/1018, Loss: 0.2508, Time: 3.85s\n",
            "Batch 260/1018, Loss: 0.2301, Time: 3.05s\n",
            "Batch 270/1018, Loss: 0.3043, Time: 3.57s\n",
            "Batch 280/1018, Loss: 0.2153, Time: 4.03s\n",
            "Batch 290/1018, Loss: 0.2777, Time: 3.40s\n",
            "Batch 300/1018, Loss: 0.2222, Time: 3.26s\n",
            "Batch 310/1018, Loss: 0.2600, Time: 3.24s\n",
            "Batch 320/1018, Loss: 0.2246, Time: 3.99s\n",
            "Batch 330/1018, Loss: 0.2360, Time: 3.31s\n",
            "Batch 340/1018, Loss: 0.2676, Time: 3.41s\n",
            "Batch 350/1018, Loss: 0.2266, Time: 3.83s\n",
            "Batch 360/1018, Loss: 0.2533, Time: 4.05s\n",
            "Batch 370/1018, Loss: 0.2323, Time: 3.38s\n",
            "Batch 380/1018, Loss: 0.2340, Time: 3.30s\n",
            "Batch 390/1018, Loss: 0.2444, Time: 3.79s\n",
            "Batch 400/1018, Loss: 0.2152, Time: 3.59s\n",
            "Batch 410/1018, Loss: 0.2383, Time: 3.40s\n",
            "Batch 420/1018, Loss: 0.2089, Time: 3.20s\n",
            "Batch 430/1018, Loss: 0.2720, Time: 3.98s\n",
            "Batch 440/1018, Loss: 0.2480, Time: 3.35s\n",
            "Batch 450/1018, Loss: 0.2097, Time: 3.13s\n",
            "Batch 460/1018, Loss: 0.2088, Time: 3.30s\n",
            "Batch 470/1018, Loss: 0.2438, Time: 4.23s\n",
            "Batch 480/1018, Loss: 0.2301, Time: 3.91s\n",
            "Batch 490/1018, Loss: 0.2691, Time: 2.86s\n",
            "Batch 500/1018, Loss: 0.2790, Time: 4.22s\n",
            "Batch 510/1018, Loss: 0.2636, Time: 3.54s\n",
            "Batch 520/1018, Loss: 0.2046, Time: 3.83s\n",
            "Batch 530/1018, Loss: 0.2112, Time: 3.44s\n",
            "Batch 540/1018, Loss: 0.2827, Time: 4.31s\n",
            "Batch 550/1018, Loss: 0.2190, Time: 3.07s\n",
            "Batch 560/1018, Loss: 0.2150, Time: 3.29s\n",
            "Batch 570/1018, Loss: 0.2843, Time: 3.78s\n",
            "Batch 580/1018, Loss: 0.2417, Time: 3.38s\n",
            "Batch 590/1018, Loss: 0.2626, Time: 3.21s\n",
            "Batch 600/1018, Loss: 0.2227, Time: 3.42s\n",
            "Batch 610/1018, Loss: 0.2258, Time: 4.02s\n",
            "Batch 620/1018, Loss: 0.2198, Time: 3.39s\n",
            "Batch 630/1018, Loss: 0.2563, Time: 3.16s\n",
            "Batch 640/1018, Loss: 0.2718, Time: 2.95s\n",
            "Batch 650/1018, Loss: 0.2622, Time: 4.02s\n",
            "Batch 660/1018, Loss: 0.2387, Time: 3.38s\n",
            "Batch 670/1018, Loss: 0.2246, Time: 3.04s\n",
            "Batch 680/1018, Loss: 0.2265, Time: 3.30s\n",
            "Batch 690/1018, Loss: 0.2438, Time: 4.12s\n",
            "Batch 700/1018, Loss: 0.2157, Time: 3.10s\n",
            "Batch 710/1018, Loss: 0.2331, Time: 3.79s\n",
            "Batch 720/1018, Loss: 0.2853, Time: 3.92s\n",
            "Batch 730/1018, Loss: 0.2650, Time: 3.68s\n",
            "Batch 740/1018, Loss: 0.3014, Time: 3.83s\n",
            "Batch 750/1018, Loss: 0.2633, Time: 3.48s\n",
            "Batch 760/1018, Loss: 0.2379, Time: 3.93s\n",
            "Batch 770/1018, Loss: 0.2286, Time: 3.26s\n",
            "Batch 780/1018, Loss: 0.2455, Time: 3.65s\n",
            "Batch 790/1018, Loss: 0.2858, Time: 3.39s\n",
            "Batch 800/1018, Loss: 0.2610, Time: 3.88s\n",
            "Batch 810/1018, Loss: 0.2615, Time: 3.73s\n",
            "Batch 820/1018, Loss: 0.2553, Time: 3.22s\n",
            "Batch 830/1018, Loss: 0.2430, Time: 4.05s\n",
            "Batch 840/1018, Loss: 0.2621, Time: 3.79s\n",
            "Batch 850/1018, Loss: 0.2960, Time: 3.27s\n",
            "Batch 860/1018, Loss: 0.2219, Time: 3.50s\n",
            "Batch 870/1018, Loss: 0.2004, Time: 3.97s\n",
            "Batch 880/1018, Loss: 0.2549, Time: 3.11s\n",
            "Batch 890/1018, Loss: 0.2501, Time: 3.25s\n",
            "Batch 900/1018, Loss: 0.2187, Time: 3.84s\n",
            "Batch 910/1018, Loss: 0.2613, Time: 4.23s\n",
            "Batch 920/1018, Loss: 0.2109, Time: 3.08s\n",
            "Batch 930/1018, Loss: 0.2313, Time: 3.18s\n",
            "Batch 940/1018, Loss: 0.2246, Time: 4.36s\n",
            "Batch 950/1018, Loss: 0.2483, Time: 3.26s\n",
            "Batch 960/1018, Loss: 0.2780, Time: 3.83s\n",
            "Batch 970/1018, Loss: 0.2591, Time: 3.11s\n",
            "Batch 980/1018, Loss: 0.2176, Time: 4.31s\n",
            "Batch 990/1018, Loss: 0.2297, Time: 3.32s\n",
            "Batch 1000/1018, Loss: 0.2556, Time: 3.55s\n",
            "Batch 1010/1018, Loss: 0.2100, Time: 3.44s\n",
            "Train Loss: 0.2413, Val Loss: 0.2372\n",
            "\n",
            "Epoch 4/10\n",
            "------------------------------\n",
            "Batch 10/1018, Loss: 0.3024, Time: 4.05s\n",
            "Batch 20/1018, Loss: 0.2235, Time: 3.84s\n",
            "Batch 30/1018, Loss: 0.2736, Time: 3.70s\n",
            "Batch 40/1018, Loss: 0.2660, Time: 3.20s\n",
            "Batch 50/1018, Loss: 0.2211, Time: 3.59s\n",
            "Batch 60/1018, Loss: 0.2410, Time: 3.95s\n",
            "Batch 70/1018, Loss: 0.2329, Time: 3.68s\n",
            "Batch 80/1018, Loss: 0.2242, Time: 3.17s\n",
            "Batch 90/1018, Loss: 0.2376, Time: 3.28s\n",
            "Batch 100/1018, Loss: 0.2801, Time: 4.33s\n",
            "Batch 110/1018, Loss: 0.2396, Time: 2.97s\n",
            "Batch 120/1018, Loss: 0.2697, Time: 3.10s\n",
            "Batch 130/1018, Loss: 0.2182, Time: 4.15s\n",
            "Batch 140/1018, Loss: 0.2423, Time: 4.40s\n",
            "Batch 150/1018, Loss: 0.2372, Time: 3.32s\n",
            "Batch 160/1018, Loss: 0.2796, Time: 3.20s\n",
            "Batch 170/1018, Loss: 0.2423, Time: 4.27s\n",
            "Batch 180/1018, Loss: 0.2642, Time: 3.52s\n",
            "Batch 190/1018, Loss: 0.2042, Time: 3.21s\n",
            "Batch 200/1018, Loss: 0.2465, Time: 3.74s\n",
            "Batch 210/1018, Loss: 0.2568, Time: 4.35s\n",
            "Batch 220/1018, Loss: 0.2142, Time: 3.14s\n",
            "Batch 230/1018, Loss: 0.2247, Time: 3.37s\n",
            "Batch 240/1018, Loss: 0.2668, Time: 3.29s\n",
            "Batch 250/1018, Loss: 0.3046, Time: 3.82s\n",
            "Batch 260/1018, Loss: 0.2461, Time: 3.53s\n",
            "Batch 270/1018, Loss: 0.2743, Time: 3.17s\n",
            "Batch 280/1018, Loss: 0.2305, Time: 3.74s\n",
            "Batch 290/1018, Loss: 0.2613, Time: 3.97s\n",
            "Batch 300/1018, Loss: 0.2295, Time: 3.32s\n",
            "Batch 310/1018, Loss: 0.2690, Time: 3.38s\n",
            "Batch 320/1018, Loss: 0.2365, Time: 4.39s\n",
            "Batch 330/1018, Loss: 0.2772, Time: 3.31s\n",
            "Batch 340/1018, Loss: 0.2331, Time: 3.09s\n",
            "Batch 350/1018, Loss: 0.2398, Time: 2.91s\n",
            "Batch 360/1018, Loss: 0.2381, Time: 3.91s\n",
            "Batch 370/1018, Loss: 0.2741, Time: 3.62s\n",
            "Batch 380/1018, Loss: 0.2643, Time: 3.86s\n",
            "Batch 390/1018, Loss: 0.2372, Time: 3.58s\n",
            "Batch 400/1018, Loss: 0.2277, Time: 3.88s\n",
            "Batch 410/1018, Loss: 0.2456, Time: 3.36s\n",
            "Batch 420/1018, Loss: 0.2427, Time: 3.38s\n",
            "Batch 430/1018, Loss: 0.2509, Time: 4.43s\n",
            "Batch 440/1018, Loss: 0.2211, Time: 3.21s\n",
            "Batch 450/1018, Loss: 0.2032, Time: 2.99s\n",
            "Batch 460/1018, Loss: 0.2733, Time: 3.02s\n",
            "Batch 470/1018, Loss: 0.2557, Time: 4.45s\n",
            "Batch 480/1018, Loss: 0.2549, Time: 2.90s\n",
            "Batch 490/1018, Loss: 0.2213, Time: 3.05s\n",
            "Batch 500/1018, Loss: 0.2484, Time: 3.18s\n",
            "Batch 510/1018, Loss: 0.2644, Time: 4.33s\n",
            "Batch 520/1018, Loss: 0.2688, Time: 3.03s\n",
            "Batch 530/1018, Loss: 0.2816, Time: 3.32s\n",
            "Batch 540/1018, Loss: 0.2547, Time: 3.80s\n",
            "Batch 550/1018, Loss: 0.2274, Time: 3.63s\n",
            "Batch 560/1018, Loss: 0.2701, Time: 3.34s\n",
            "Batch 570/1018, Loss: 0.2530, Time: 3.34s\n",
            "Batch 580/1018, Loss: 0.1952, Time: 3.68s\n",
            "Batch 590/1018, Loss: 0.2617, Time: 3.52s\n",
            "Batch 600/1018, Loss: 0.2704, Time: 3.31s\n",
            "Batch 610/1018, Loss: 0.2565, Time: 3.57s\n",
            "Batch 620/1018, Loss: 0.2640, Time: 3.96s\n",
            "Batch 630/1018, Loss: 0.2803, Time: 3.96s\n",
            "Batch 640/1018, Loss: 0.2581, Time: 3.28s\n",
            "Batch 650/1018, Loss: 0.2839, Time: 3.70s\n",
            "Batch 660/1018, Loss: 0.2762, Time: 4.01s\n",
            "Batch 670/1018, Loss: 0.2805, Time: 3.10s\n",
            "Batch 680/1018, Loss: 0.2586, Time: 3.02s\n",
            "Batch 690/1018, Loss: 0.2816, Time: 3.43s\n",
            "Batch 700/1018, Loss: 0.2435, Time: 3.53s\n",
            "Batch 710/1018, Loss: 0.1806, Time: 4.25s\n",
            "Batch 720/1018, Loss: 0.2710, Time: 3.44s\n",
            "Batch 730/1018, Loss: 0.2074, Time: 4.39s\n",
            "Batch 740/1018, Loss: 0.2569, Time: 3.40s\n",
            "Batch 750/1018, Loss: 0.2472, Time: 3.01s\n",
            "Batch 760/1018, Loss: 0.2551, Time: 4.10s\n",
            "Batch 770/1018, Loss: 0.2141, Time: 3.88s\n",
            "Batch 780/1018, Loss: 0.2189, Time: 3.25s\n",
            "Batch 790/1018, Loss: 0.2439, Time: 3.45s\n",
            "Batch 800/1018, Loss: 0.2293, Time: 4.26s\n",
            "Batch 810/1018, Loss: 0.2216, Time: 3.18s\n",
            "Batch 820/1018, Loss: 0.2455, Time: 3.33s\n",
            "Batch 830/1018, Loss: 0.2392, Time: 3.56s\n",
            "Batch 840/1018, Loss: 0.2439, Time: 4.49s\n",
            "Batch 850/1018, Loss: 0.2273, Time: 3.30s\n",
            "Batch 860/1018, Loss: 0.2400, Time: 3.43s\n",
            "Batch 870/1018, Loss: 0.2153, Time: 4.44s\n",
            "Batch 880/1018, Loss: 0.1607, Time: 3.56s\n",
            "Batch 890/1018, Loss: 0.2419, Time: 3.76s\n",
            "Batch 900/1018, Loss: 0.2322, Time: 3.41s\n",
            "Batch 910/1018, Loss: 0.2469, Time: 4.64s\n",
            "Batch 920/1018, Loss: 0.1979, Time: 3.50s\n",
            "Batch 930/1018, Loss: 0.2344, Time: 3.82s\n",
            "Batch 940/1018, Loss: 0.2625, Time: 4.18s\n",
            "Batch 950/1018, Loss: 0.2571, Time: 3.51s\n",
            "Batch 960/1018, Loss: 0.2825, Time: 3.25s\n",
            "Batch 970/1018, Loss: 0.2362, Time: 3.31s\n",
            "Batch 980/1018, Loss: 0.2569, Time: 4.62s\n",
            "Batch 990/1018, Loss: 0.2607, Time: 3.24s\n",
            "Batch 1000/1018, Loss: 0.2599, Time: 3.21s\n",
            "Batch 1010/1018, Loss: 0.2386, Time: 3.50s\n",
            "Train Loss: 0.2445, Val Loss: 0.2430\n",
            "\n",
            "Epoch 5/10\n",
            "------------------------------\n",
            "Batch 10/1018, Loss: 0.2540, Time: 3.44s\n",
            "Batch 20/1018, Loss: 0.2737, Time: 4.04s\n",
            "Batch 30/1018, Loss: 0.2737, Time: 3.71s\n",
            "Batch 40/1018, Loss: 0.2638, Time: 3.30s\n",
            "Batch 50/1018, Loss: 0.2614, Time: 3.15s\n",
            "Batch 60/1018, Loss: 0.2143, Time: 4.41s\n",
            "Batch 70/1018, Loss: 0.2198, Time: 3.46s\n",
            "Batch 80/1018, Loss: 0.3184, Time: 3.30s\n",
            "Batch 90/1018, Loss: 0.2151, Time: 4.05s\n",
            "Batch 100/1018, Loss: 0.2805, Time: 3.68s\n",
            "Batch 110/1018, Loss: 0.2273, Time: 3.77s\n",
            "Batch 120/1018, Loss: 0.2813, Time: 4.27s\n",
            "Batch 130/1018, Loss: 0.2513, Time: 4.44s\n",
            "Batch 140/1018, Loss: 0.2782, Time: 3.62s\n",
            "Batch 150/1018, Loss: 0.1973, Time: 2.99s\n",
            "Batch 160/1018, Loss: 0.2111, Time: 4.00s\n",
            "Batch 170/1018, Loss: 0.2515, Time: 3.83s\n",
            "Batch 180/1018, Loss: 0.2116, Time: 3.23s\n",
            "Batch 190/1018, Loss: 0.2548, Time: 3.49s\n",
            "Batch 200/1018, Loss: 0.2263, Time: 4.04s\n",
            "Batch 210/1018, Loss: 0.2670, Time: 3.21s\n",
            "Batch 220/1018, Loss: 0.2271, Time: 3.43s\n",
            "Batch 230/1018, Loss: 0.2832, Time: 3.62s\n",
            "Batch 240/1018, Loss: 0.2794, Time: 4.21s\n",
            "Batch 250/1018, Loss: 0.2998, Time: 3.26s\n",
            "Batch 260/1018, Loss: 0.1997, Time: 3.29s\n",
            "Batch 270/1018, Loss: 0.2323, Time: 3.82s\n",
            "Batch 280/1018, Loss: 0.2525, Time: 3.70s\n",
            "Batch 290/1018, Loss: 0.2521, Time: 3.63s\n",
            "Batch 300/1018, Loss: 0.2737, Time: 3.23s\n",
            "Batch 310/1018, Loss: 0.2711, Time: 4.39s\n",
            "Batch 320/1018, Loss: 0.2074, Time: 3.30s\n",
            "Batch 330/1018, Loss: 0.2394, Time: 3.24s\n",
            "Batch 340/1018, Loss: 0.2457, Time: 3.52s\n",
            "Batch 350/1018, Loss: 0.2592, Time: 3.63s\n",
            "Batch 360/1018, Loss: 0.1801, Time: 3.44s\n",
            "Batch 370/1018, Loss: 0.2271, Time: 3.29s\n",
            "Batch 380/1018, Loss: 0.2402, Time: 4.06s\n",
            "Batch 390/1018, Loss: 0.2802, Time: 3.27s\n",
            "Batch 400/1018, Loss: 0.2137, Time: 3.68s\n",
            "Batch 410/1018, Loss: 0.2444, Time: 3.33s\n",
            "Batch 420/1018, Loss: 0.2570, Time: 4.16s\n",
            "Batch 430/1018, Loss: 0.2301, Time: 3.27s\n",
            "Batch 440/1018, Loss: 0.2427, Time: 2.89s\n",
            "Batch 450/1018, Loss: 0.2678, Time: 3.41s\n",
            "Batch 460/1018, Loss: 0.2684, Time: 3.94s\n",
            "Batch 470/1018, Loss: 0.2206, Time: 3.29s\n",
            "Batch 480/1018, Loss: 0.2431, Time: 3.81s\n",
            "Batch 490/1018, Loss: 0.2392, Time: 3.92s\n",
            "Batch 500/1018, Loss: 0.2163, Time: 3.51s\n",
            "Batch 510/1018, Loss: 0.2289, Time: 3.63s\n",
            "Batch 520/1018, Loss: 0.2023, Time: 3.29s\n",
            "Batch 530/1018, Loss: 0.2478, Time: 4.13s\n",
            "Batch 540/1018, Loss: 0.2585, Time: 3.15s\n",
            "Batch 550/1018, Loss: 0.2127, Time: 3.53s\n",
            "Batch 560/1018, Loss: 0.2738, Time: 3.31s\n",
            "Batch 570/1018, Loss: 0.2279, Time: 3.67s\n",
            "Batch 580/1018, Loss: 0.2522, Time: 3.90s\n",
            "Batch 590/1018, Loss: 0.2638, Time: 3.58s\n",
            "Batch 600/1018, Loss: 0.2315, Time: 3.90s\n",
            "Batch 610/1018, Loss: 0.2281, Time: 3.22s\n",
            "Batch 620/1018, Loss: 0.2370, Time: 3.19s\n",
            "Batch 630/1018, Loss: 0.2498, Time: 3.30s\n",
            "Batch 640/1018, Loss: 0.2055, Time: 4.10s\n",
            "Batch 650/1018, Loss: 0.2291, Time: 3.32s\n",
            "Batch 660/1018, Loss: 0.2336, Time: 3.60s\n",
            "Batch 670/1018, Loss: 0.2123, Time: 3.64s\n",
            "Batch 680/1018, Loss: 0.2655, Time: 3.69s\n",
            "Batch 690/1018, Loss: 0.2114, Time: 3.24s\n",
            "Batch 700/1018, Loss: 0.2333, Time: 4.24s\n",
            "Batch 710/1018, Loss: 0.1807, Time: 4.10s\n",
            "Batch 720/1018, Loss: 0.1962, Time: 3.72s\n",
            "Batch 730/1018, Loss: 0.2771, Time: 3.27s\n",
            "Batch 740/1018, Loss: 0.2022, Time: 3.56s\n",
            "Batch 750/1018, Loss: 0.2204, Time: 4.44s\n",
            "Batch 760/1018, Loss: 0.2234, Time: 3.17s\n",
            "Batch 770/1018, Loss: 0.2165, Time: 3.12s\n",
            "Batch 780/1018, Loss: 0.2314, Time: 3.28s\n",
            "Batch 790/1018, Loss: 0.2481, Time: 3.82s\n",
            "Batch 800/1018, Loss: 0.2119, Time: 3.43s\n",
            "Batch 810/1018, Loss: 0.2816, Time: 4.07s\n",
            "Batch 820/1018, Loss: 0.2413, Time: 3.90s\n",
            "Batch 830/1018, Loss: 0.2573, Time: 3.26s\n",
            "Batch 840/1018, Loss: 0.2048, Time: 2.99s\n",
            "Batch 850/1018, Loss: 0.1979, Time: 4.03s\n",
            "Batch 860/1018, Loss: 0.2048, Time: 4.01s\n",
            "Batch 870/1018, Loss: 0.2618, Time: 3.47s\n",
            "Batch 880/1018, Loss: 0.2681, Time: 3.24s\n",
            "Batch 890/1018, Loss: 0.2330, Time: 3.89s\n",
            "Batch 900/1018, Loss: 0.2489, Time: 3.37s\n",
            "Batch 910/1018, Loss: 0.2452, Time: 3.46s\n",
            "Batch 920/1018, Loss: 0.2606, Time: 3.12s\n",
            "Batch 930/1018, Loss: 0.1911, Time: 3.96s\n",
            "Batch 940/1018, Loss: 0.2821, Time: 3.36s\n",
            "Batch 950/1018, Loss: 0.2247, Time: 3.13s\n",
            "Batch 960/1018, Loss: 0.2782, Time: 3.08s\n",
            "Batch 970/1018, Loss: 0.2194, Time: 4.20s\n",
            "Batch 980/1018, Loss: 0.2325, Time: 3.71s\n",
            "Batch 990/1018, Loss: 0.1809, Time: 3.19s\n",
            "Batch 1000/1018, Loss: 0.2246, Time: 3.97s\n",
            "Batch 1010/1018, Loss: 0.2298, Time: 3.62s\n",
            "Train Loss: 0.2405, Val Loss: 0.2384\n",
            "\n",
            "Epoch 6/10\n",
            "------------------------------\n",
            "Batch 10/1018, Loss: 0.2277, Time: 3.71s\n",
            "Batch 20/1018, Loss: 0.2400, Time: 3.77s\n",
            "Batch 30/1018, Loss: 0.2606, Time: 3.39s\n",
            "Batch 40/1018, Loss: 0.2406, Time: 2.99s\n",
            "Batch 50/1018, Loss: 0.2549, Time: 4.29s\n",
            "Batch 60/1018, Loss: 0.2678, Time: 3.34s\n",
            "Batch 70/1018, Loss: 0.2496, Time: 3.30s\n",
            "Batch 80/1018, Loss: 0.2193, Time: 3.48s\n",
            "Batch 90/1018, Loss: 0.2591, Time: 4.07s\n",
            "Batch 100/1018, Loss: 0.2224, Time: 3.42s\n",
            "Batch 110/1018, Loss: 0.2440, Time: 3.16s\n",
            "Batch 120/1018, Loss: 0.2053, Time: 3.34s\n",
            "Batch 130/1018, Loss: 0.2296, Time: 5.02s\n",
            "Batch 140/1018, Loss: 0.2417, Time: 3.49s\n",
            "Batch 150/1018, Loss: 0.2270, Time: 3.17s\n",
            "Batch 160/1018, Loss: 0.2228, Time: 4.15s\n",
            "Batch 170/1018, Loss: 0.2570, Time: 3.19s\n",
            "Batch 180/1018, Loss: 0.2801, Time: 3.99s\n",
            "Batch 190/1018, Loss: 0.1930, Time: 3.67s\n",
            "Batch 200/1018, Loss: 0.2186, Time: 3.51s\n",
            "Batch 210/1018, Loss: 0.2349, Time: 3.36s\n",
            "Batch 220/1018, Loss: 0.2466, Time: 3.34s\n",
            "Batch 230/1018, Loss: 0.2684, Time: 4.01s\n",
            "Batch 240/1018, Loss: 0.2640, Time: 4.05s\n",
            "Batch 250/1018, Loss: 0.1844, Time: 3.17s\n",
            "Batch 260/1018, Loss: 0.2865, Time: 3.08s\n",
            "Batch 270/1018, Loss: 0.2212, Time: 4.06s\n",
            "Batch 280/1018, Loss: 0.2358, Time: 3.23s\n",
            "Batch 290/1018, Loss: 0.2537, Time: 3.48s\n",
            "Batch 300/1018, Loss: 0.2674, Time: 3.35s\n",
            "Batch 310/1018, Loss: 0.2787, Time: 4.06s\n",
            "Batch 320/1018, Loss: 0.2651, Time: 3.12s\n",
            "Batch 330/1018, Loss: 0.2752, Time: 3.24s\n",
            "Batch 340/1018, Loss: 0.2207, Time: 3.69s\n",
            "Batch 350/1018, Loss: 0.2453, Time: 3.54s\n",
            "Batch 360/1018, Loss: 0.2318, Time: 3.45s\n",
            "Batch 370/1018, Loss: 0.2414, Time: 3.56s\n",
            "Batch 380/1018, Loss: 0.2784, Time: 4.33s\n",
            "Batch 390/1018, Loss: 0.1990, Time: 3.58s\n",
            "Batch 400/1018, Loss: 0.2564, Time: 3.39s\n",
            "Batch 410/1018, Loss: 0.2503, Time: 3.51s\n",
            "Batch 420/1018, Loss: 0.2323, Time: 4.04s\n",
            "Batch 430/1018, Loss: 0.2439, Time: 3.56s\n",
            "Batch 440/1018, Loss: 0.2633, Time: 3.60s\n",
            "Batch 450/1018, Loss: 0.2482, Time: 3.96s\n",
            "Batch 460/1018, Loss: 0.2231, Time: 3.47s\n",
            "Batch 470/1018, Loss: 0.1957, Time: 3.53s\n",
            "Batch 480/1018, Loss: 0.2375, Time: 3.45s\n",
            "Batch 490/1018, Loss: 0.2150, Time: 4.14s\n",
            "Batch 500/1018, Loss: 0.2371, Time: 3.34s\n",
            "Batch 510/1018, Loss: 0.2442, Time: 3.27s\n",
            "Batch 520/1018, Loss: 0.2197, Time: 3.52s\n",
            "Batch 530/1018, Loss: 0.2673, Time: 3.97s\n",
            "Batch 540/1018, Loss: 0.2243, Time: 3.47s\n",
            "Batch 550/1018, Loss: 0.2344, Time: 3.44s\n",
            "Batch 560/1018, Loss: 0.2350, Time: 4.43s\n",
            "Batch 570/1018, Loss: 0.2653, Time: 3.23s\n",
            "Batch 580/1018, Loss: 0.2487, Time: 3.06s\n",
            "Batch 590/1018, Loss: 0.2448, Time: 3.17s\n",
            "Batch 600/1018, Loss: 0.2333, Time: 4.09s\n",
            "Batch 610/1018, Loss: 0.2325, Time: 3.38s\n",
            "Batch 620/1018, Loss: 0.2621, Time: 3.24s\n",
            "Batch 630/1018, Loss: 0.2399, Time: 3.66s\n",
            "Batch 640/1018, Loss: 0.1810, Time: 3.69s\n",
            "Batch 650/1018, Loss: 0.2262, Time: 3.29s\n",
            "Batch 660/1018, Loss: 0.2462, Time: 3.26s\n",
            "Batch 670/1018, Loss: 0.1871, Time: 3.55s\n",
            "Batch 680/1018, Loss: 0.2432, Time: 3.52s\n",
            "Batch 690/1018, Loss: 0.2669, Time: 3.14s\n",
            "Batch 700/1018, Loss: 0.2392, Time: 4.21s\n",
            "Batch 710/1018, Loss: 0.2334, Time: 4.10s\n",
            "Batch 720/1018, Loss: 0.2636, Time: 3.46s\n",
            "Batch 730/1018, Loss: 0.1946, Time: 3.77s\n",
            "Batch 740/1018, Loss: 0.2634, Time: 3.23s\n",
            "Batch 750/1018, Loss: 0.2405, Time: 4.16s\n",
            "Batch 760/1018, Loss: 0.2085, Time: 3.43s\n",
            "Batch 770/1018, Loss: 0.2110, Time: 3.52s\n",
            "Batch 780/1018, Loss: 0.2431, Time: 3.99s\n",
            "Batch 790/1018, Loss: 0.1914, Time: 3.35s\n",
            "Batch 800/1018, Loss: 0.2256, Time: 3.73s\n",
            "Batch 810/1018, Loss: 0.2314, Time: 3.76s\n",
            "Batch 820/1018, Loss: 0.2842, Time: 3.94s\n",
            "Batch 830/1018, Loss: 0.2220, Time: 3.56s\n",
            "Batch 840/1018, Loss: 0.2286, Time: 3.00s\n",
            "Batch 850/1018, Loss: 0.2470, Time: 3.41s\n",
            "Batch 860/1018, Loss: 0.2672, Time: 3.69s\n",
            "Batch 870/1018, Loss: 0.2382, Time: 3.02s\n",
            "Batch 880/1018, Loss: 0.2496, Time: 3.44s\n",
            "Batch 890/1018, Loss: 0.1985, Time: 4.00s\n",
            "Batch 900/1018, Loss: 0.2703, Time: 3.99s\n",
            "Batch 910/1018, Loss: 0.2413, Time: 3.83s\n",
            "Batch 920/1018, Loss: 0.2392, Time: 3.57s\n",
            "Batch 930/1018, Loss: 0.2560, Time: 4.07s\n",
            "Batch 940/1018, Loss: 0.2619, Time: 3.09s\n",
            "Batch 950/1018, Loss: 0.1962, Time: 3.29s\n",
            "Batch 960/1018, Loss: 0.2026, Time: 3.54s\n",
            "Batch 970/1018, Loss: 0.2721, Time: 3.53s\n",
            "Batch 980/1018, Loss: 0.2668, Time: 3.09s\n",
            "Batch 990/1018, Loss: 0.2732, Time: 3.58s\n",
            "Batch 1000/1018, Loss: 0.2973, Time: 4.05s\n",
            "Batch 1010/1018, Loss: 0.2456, Time: 3.35s\n",
            "Train Loss: 0.2394, Val Loss: 0.2381\n",
            "\n",
            "Epoch 7/10\n",
            "------------------------------\n",
            "Batch 10/1018, Loss: 0.2080, Time: 3.85s\n",
            "Batch 20/1018, Loss: 0.2143, Time: 3.49s\n",
            "Batch 30/1018, Loss: 0.2186, Time: 3.47s\n",
            "Batch 40/1018, Loss: 0.2328, Time: 3.14s\n",
            "Batch 50/1018, Loss: 0.3030, Time: 4.53s\n",
            "Batch 60/1018, Loss: 0.2435, Time: 3.44s\n",
            "Batch 70/1018, Loss: 0.1998, Time: 3.25s\n",
            "Batch 80/1018, Loss: 0.2837, Time: 3.16s\n",
            "Batch 90/1018, Loss: 0.2385, Time: 4.25s\n",
            "Batch 100/1018, Loss: 0.2372, Time: 3.18s\n",
            "Batch 110/1018, Loss: 0.2256, Time: 3.18s\n",
            "Batch 120/1018, Loss: 0.2190, Time: 3.58s\n",
            "Batch 130/1018, Loss: 0.2036, Time: 3.75s\n",
            "Batch 140/1018, Loss: 0.2161, Time: 3.90s\n",
            "Batch 150/1018, Loss: 0.2332, Time: 3.12s\n",
            "Batch 160/1018, Loss: 0.2212, Time: 4.16s\n",
            "Batch 170/1018, Loss: 0.2423, Time: 3.20s\n",
            "Batch 180/1018, Loss: 0.1822, Time: 3.29s\n",
            "Batch 190/1018, Loss: 0.2667, Time: 3.33s\n",
            "Batch 200/1018, Loss: 0.2665, Time: 4.31s\n",
            "Batch 210/1018, Loss: 0.2487, Time: 3.17s\n",
            "Batch 220/1018, Loss: 0.2684, Time: 3.45s\n",
            "Batch 230/1018, Loss: 0.2629, Time: 3.84s\n",
            "Batch 240/1018, Loss: 0.2406, Time: 3.43s\n",
            "Batch 250/1018, Loss: 0.2561, Time: 3.25s\n",
            "Batch 260/1018, Loss: 0.2641, Time: 3.18s\n",
            "Batch 270/1018, Loss: 0.2118, Time: 3.94s\n",
            "Batch 280/1018, Loss: 0.2646, Time: 3.75s\n",
            "Batch 290/1018, Loss: 0.2464, Time: 3.39s\n",
            "Batch 300/1018, Loss: 0.2301, Time: 3.65s\n",
            "Batch 310/1018, Loss: 0.2622, Time: 4.05s\n",
            "Batch 320/1018, Loss: 0.2358, Time: 3.31s\n",
            "Batch 330/1018, Loss: 0.2420, Time: 3.41s\n",
            "Batch 340/1018, Loss: 0.2865, Time: 3.97s\n",
            "Batch 350/1018, Loss: 0.2190, Time: 3.60s\n",
            "Batch 360/1018, Loss: 0.2352, Time: 3.37s\n",
            "Batch 370/1018, Loss: 0.2629, Time: 3.88s\n",
            "Batch 380/1018, Loss: 0.2208, Time: 3.94s\n",
            "Batch 390/1018, Loss: 0.2037, Time: 3.44s\n",
            "Batch 400/1018, Loss: 0.2504, Time: 3.34s\n",
            "Batch 410/1018, Loss: 0.2163, Time: 4.10s\n",
            "Batch 420/1018, Loss: 0.2580, Time: 3.86s\n",
            "Batch 430/1018, Loss: 0.2174, Time: 3.28s\n",
            "Batch 440/1018, Loss: 0.2542, Time: 3.53s\n",
            "Batch 450/1018, Loss: 0.2529, Time: 4.77s\n",
            "Batch 460/1018, Loss: 0.2692, Time: 3.82s\n",
            "Batch 470/1018, Loss: 0.2395, Time: 3.33s\n",
            "Batch 480/1018, Loss: 0.2153, Time: 3.74s\n",
            "Batch 490/1018, Loss: 0.2219, Time: 3.87s\n",
            "Batch 500/1018, Loss: 0.2449, Time: 3.27s\n",
            "Batch 510/1018, Loss: 0.2200, Time: 3.01s\n",
            "Batch 520/1018, Loss: 0.2053, Time: 3.93s\n",
            "Batch 530/1018, Loss: 0.2495, Time: 3.72s\n",
            "Batch 540/1018, Loss: 0.2174, Time: 3.08s\n",
            "Batch 550/1018, Loss: 0.1874, Time: 3.43s\n",
            "Batch 560/1018, Loss: 0.2024, Time: 4.43s\n",
            "Batch 570/1018, Loss: 0.2271, Time: 3.30s\n",
            "Batch 580/1018, Loss: 0.2391, Time: 3.37s\n",
            "Batch 590/1018, Loss: 0.2330, Time: 3.63s\n",
            "Batch 600/1018, Loss: 0.2680, Time: 3.85s\n",
            "Batch 610/1018, Loss: 0.3040, Time: 3.31s\n",
            "Batch 620/1018, Loss: 0.2029, Time: 3.15s\n",
            "Batch 630/1018, Loss: 0.2138, Time: 3.59s\n",
            "Batch 640/1018, Loss: 0.2193, Time: 3.33s\n",
            "Batch 650/1018, Loss: 0.1883, Time: 3.22s\n",
            "Batch 660/1018, Loss: 0.2456, Time: 3.48s\n",
            "Batch 670/1018, Loss: 0.2569, Time: 3.63s\n",
            "Batch 680/1018, Loss: 0.2276, Time: 3.63s\n",
            "Batch 690/1018, Loss: 0.2515, Time: 3.13s\n",
            "Batch 700/1018, Loss: 0.2746, Time: 3.20s\n",
            "Batch 710/1018, Loss: 0.2226, Time: 4.03s\n",
            "Batch 720/1018, Loss: 0.1824, Time: 3.20s\n",
            "Batch 730/1018, Loss: 0.2580, Time: 3.07s\n",
            "Batch 740/1018, Loss: 0.2355, Time: 3.21s\n",
            "Batch 750/1018, Loss: 0.2037, Time: 4.19s\n",
            "Batch 760/1018, Loss: 0.2374, Time: 2.99s\n",
            "Batch 770/1018, Loss: 0.2633, Time: 3.02s\n",
            "Batch 780/1018, Loss: 0.2472, Time: 3.50s\n",
            "Batch 790/1018, Loss: 0.2621, Time: 4.12s\n",
            "Batch 800/1018, Loss: 0.2354, Time: 3.32s\n",
            "Batch 810/1018, Loss: 0.1871, Time: 3.49s\n",
            "Batch 820/1018, Loss: 0.2408, Time: 3.46s\n",
            "Batch 830/1018, Loss: 0.2490, Time: 4.12s\n",
            "Batch 840/1018, Loss: 0.2017, Time: 3.30s\n",
            "Batch 850/1018, Loss: 0.2702, Time: 3.36s\n",
            "Batch 860/1018, Loss: 0.2236, Time: 4.43s\n",
            "Batch 870/1018, Loss: 0.2260, Time: 3.10s\n",
            "Batch 880/1018, Loss: 0.2027, Time: 3.19s\n",
            "Batch 890/1018, Loss: 0.2728, Time: 3.20s\n",
            "Batch 900/1018, Loss: 0.2540, Time: 4.19s\n",
            "Batch 910/1018, Loss: 0.2000, Time: 3.22s\n",
            "Batch 920/1018, Loss: 0.2204, Time: 3.20s\n",
            "Batch 930/1018, Loss: 0.2480, Time: 3.66s\n",
            "Batch 940/1018, Loss: 0.2427, Time: 3.77s\n",
            "Batch 950/1018, Loss: 0.2272, Time: 3.34s\n",
            "Batch 960/1018, Loss: 0.2173, Time: 3.26s\n",
            "Batch 970/1018, Loss: 0.2495, Time: 4.67s\n",
            "Batch 980/1018, Loss: 0.1976, Time: 3.50s\n",
            "Batch 990/1018, Loss: 0.3063, Time: 3.54s\n",
            "Batch 1000/1018, Loss: 0.2328, Time: 4.33s\n",
            "Batch 1010/1018, Loss: 0.2764, Time: 3.71s\n",
            "Train Loss: 0.2364, Val Loss: 0.2357\n",
            "\n",
            "Epoch 8/10\n",
            "------------------------------\n",
            "Batch 10/1018, Loss: 0.2278, Time: 3.36s\n",
            "Batch 20/1018, Loss: 0.1966, Time: 3.63s\n",
            "Batch 30/1018, Loss: 0.2560, Time: 3.05s\n",
            "Batch 40/1018, Loss: 0.2492, Time: 3.57s\n",
            "Batch 50/1018, Loss: 0.2353, Time: 3.53s\n",
            "Batch 60/1018, Loss: 0.1889, Time: 3.58s\n",
            "Batch 70/1018, Loss: 0.2212, Time: 3.40s\n",
            "Batch 80/1018, Loss: 0.2628, Time: 4.09s\n",
            "Batch 90/1018, Loss: 0.1995, Time: 3.78s\n",
            "Batch 100/1018, Loss: 0.2459, Time: 3.45s\n",
            "Batch 110/1018, Loss: 0.2129, Time: 3.31s\n",
            "Batch 120/1018, Loss: 0.2385, Time: 3.62s\n",
            "Batch 130/1018, Loss: 0.2443, Time: 3.92s\n",
            "Batch 140/1018, Loss: 0.2445, Time: 3.51s\n",
            "Batch 150/1018, Loss: 0.2413, Time: 4.10s\n",
            "Batch 160/1018, Loss: 0.1803, Time: 5.26s\n",
            "Batch 170/1018, Loss: 0.2238, Time: 3.17s\n",
            "Batch 180/1018, Loss: 0.2354, Time: 3.20s\n",
            "Batch 190/1018, Loss: 0.2303, Time: 3.15s\n",
            "Batch 200/1018, Loss: 0.2529, Time: 3.97s\n",
            "Batch 210/1018, Loss: 0.2525, Time: 3.87s\n",
            "Batch 220/1018, Loss: 0.2105, Time: 3.91s\n",
            "Batch 230/1018, Loss: 0.2534, Time: 3.88s\n",
            "Batch 240/1018, Loss: 0.2005, Time: 3.24s\n",
            "Batch 250/1018, Loss: 0.2428, Time: 3.58s\n",
            "Batch 260/1018, Loss: 0.2249, Time: 2.99s\n",
            "Batch 270/1018, Loss: 0.2481, Time: 4.41s\n",
            "Batch 280/1018, Loss: 0.2779, Time: 3.02s\n",
            "Batch 290/1018, Loss: 0.2538, Time: 3.34s\n",
            "Batch 300/1018, Loss: 0.2769, Time: 3.27s\n",
            "Batch 310/1018, Loss: 0.2333, Time: 4.09s\n",
            "Batch 320/1018, Loss: 0.1876, Time: 3.19s\n",
            "Batch 330/1018, Loss: 0.1926, Time: 3.70s\n",
            "Batch 340/1018, Loss: 0.2793, Time: 4.37s\n",
            "Batch 350/1018, Loss: 0.2537, Time: 3.36s\n",
            "Batch 360/1018, Loss: 0.2646, Time: 3.02s\n",
            "Batch 370/1018, Loss: 0.2381, Time: 3.31s\n",
            "Batch 380/1018, Loss: 0.2394, Time: 3.90s\n",
            "Batch 390/1018, Loss: 0.2549, Time: 3.46s\n",
            "Batch 400/1018, Loss: 0.2184, Time: 3.23s\n",
            "Batch 410/1018, Loss: 0.2247, Time: 3.04s\n",
            "Batch 420/1018, Loss: 0.2159, Time: 3.96s\n",
            "Batch 430/1018, Loss: 0.2455, Time: 3.51s\n",
            "Batch 440/1018, Loss: 0.2599, Time: 3.70s\n",
            "Batch 450/1018, Loss: 0.2172, Time: 4.29s\n",
            "Batch 460/1018, Loss: 0.2116, Time: 3.69s\n",
            "Batch 470/1018, Loss: 0.2639, Time: 3.20s\n",
            "Batch 480/1018, Loss: 0.2289, Time: 3.23s\n",
            "Batch 490/1018, Loss: 0.2265, Time: 4.40s\n",
            "Batch 500/1018, Loss: 0.2460, Time: 3.51s\n",
            "Batch 510/1018, Loss: 0.2305, Time: 3.14s\n",
            "Batch 520/1018, Loss: 0.2145, Time: 3.27s\n",
            "Batch 530/1018, Loss: 0.2224, Time: 4.15s\n",
            "Batch 540/1018, Loss: 0.2454, Time: 3.48s\n",
            "Batch 550/1018, Loss: 0.2330, Time: 3.25s\n",
            "Batch 560/1018, Loss: 0.2356, Time: 3.62s\n",
            "Batch 570/1018, Loss: 0.2543, Time: 4.09s\n",
            "Batch 580/1018, Loss: 0.2297, Time: 4.15s\n",
            "Batch 590/1018, Loss: 0.2476, Time: 3.07s\n",
            "Batch 600/1018, Loss: 0.3054, Time: 4.42s\n",
            "Batch 610/1018, Loss: 0.2364, Time: 3.21s\n",
            "Batch 620/1018, Loss: 0.2270, Time: 3.42s\n",
            "Batch 630/1018, Loss: 0.2085, Time: 3.46s\n",
            "Batch 640/1018, Loss: 0.2860, Time: 3.85s\n",
            "Batch 650/1018, Loss: 0.2110, Time: 3.27s\n",
            "Batch 660/1018, Loss: 0.2810, Time: 3.20s\n",
            "Batch 670/1018, Loss: 0.2548, Time: 4.29s\n",
            "Batch 680/1018, Loss: 0.2037, Time: 3.57s\n",
            "Batch 690/1018, Loss: 0.2537, Time: 3.72s\n",
            "Batch 700/1018, Loss: 0.2463, Time: 3.54s\n",
            "Batch 710/1018, Loss: 0.2237, Time: 4.10s\n",
            "Batch 720/1018, Loss: 0.2077, Time: 3.58s\n",
            "Batch 730/1018, Loss: 0.2740, Time: 3.97s\n",
            "Batch 740/1018, Loss: 0.2065, Time: 4.87s\n",
            "Batch 750/1018, Loss: 0.2132, Time: 3.35s\n",
            "Batch 760/1018, Loss: 0.2240, Time: 3.36s\n",
            "Batch 770/1018, Loss: 0.2652, Time: 3.47s\n",
            "Batch 780/1018, Loss: 0.2665, Time: 3.98s\n",
            "Batch 790/1018, Loss: 0.2587, Time: 3.33s\n",
            "Batch 800/1018, Loss: 0.2269, Time: 3.14s\n",
            "Batch 810/1018, Loss: 0.2354, Time: 4.13s\n",
            "Batch 820/1018, Loss: 0.1944, Time: 3.15s\n",
            "Batch 830/1018, Loss: 0.2561, Time: 3.17s\n",
            "Batch 840/1018, Loss: 0.2361, Time: 3.40s\n",
            "Batch 850/1018, Loss: 0.2649, Time: 3.92s\n",
            "Batch 860/1018, Loss: 0.2631, Time: 3.33s\n",
            "Batch 870/1018, Loss: 0.2750, Time: 3.36s\n",
            "Batch 880/1018, Loss: 0.2529, Time: 3.21s\n",
            "Batch 890/1018, Loss: 0.2484, Time: 3.87s\n",
            "Batch 900/1018, Loss: 0.2422, Time: 3.54s\n",
            "Batch 910/1018, Loss: 0.2671, Time: 3.27s\n",
            "Batch 920/1018, Loss: 0.2318, Time: 3.61s\n",
            "Batch 930/1018, Loss: 0.3056, Time: 4.11s\n",
            "Batch 940/1018, Loss: 0.2654, Time: 3.02s\n",
            "Batch 950/1018, Loss: 0.2433, Time: 3.17s\n",
            "Batch 960/1018, Loss: 0.2394, Time: 3.73s\n",
            "Batch 970/1018, Loss: 0.2533, Time: 3.22s\n",
            "Batch 980/1018, Loss: 0.2204, Time: 3.48s\n",
            "Batch 990/1018, Loss: 0.2433, Time: 3.26s\n",
            "Batch 1000/1018, Loss: 0.2339, Time: 3.89s\n",
            "Batch 1010/1018, Loss: 0.2680, Time: 3.64s\n",
            "Train Loss: 0.2369, Val Loss: 0.2341\n",
            "\n",
            "Epoch 9/10\n",
            "------------------------------\n",
            "Batch 10/1018, Loss: 0.2415, Time: 4.39s\n",
            "Batch 20/1018, Loss: 0.2657, Time: 3.35s\n",
            "Batch 30/1018, Loss: 0.2360, Time: 3.70s\n",
            "Batch 40/1018, Loss: 0.2327, Time: 3.47s\n",
            "Batch 50/1018, Loss: 0.2196, Time: 4.27s\n",
            "Batch 60/1018, Loss: 0.2499, Time: 3.42s\n",
            "Batch 70/1018, Loss: 0.2640, Time: 3.18s\n",
            "Batch 80/1018, Loss: 0.2379, Time: 3.56s\n",
            "Batch 90/1018, Loss: 0.2313, Time: 3.90s\n",
            "Batch 100/1018, Loss: 0.2052, Time: 3.22s\n",
            "Batch 110/1018, Loss: 0.2790, Time: 3.23s\n",
            "Batch 120/1018, Loss: 0.2493, Time: 3.75s\n",
            "Batch 130/1018, Loss: 0.2743, Time: 3.70s\n",
            "Batch 140/1018, Loss: 0.2605, Time: 3.23s\n",
            "Batch 150/1018, Loss: 0.2434, Time: 3.54s\n",
            "Batch 160/1018, Loss: 0.2359, Time: 4.41s\n",
            "Batch 170/1018, Loss: 0.2664, Time: 3.68s\n",
            "Batch 180/1018, Loss: 0.2192, Time: 4.27s\n",
            "Batch 190/1018, Loss: 0.1949, Time: 4.11s\n",
            "Batch 200/1018, Loss: 0.2272, Time: 3.15s\n",
            "Batch 210/1018, Loss: 0.2290, Time: 3.07s\n",
            "Batch 220/1018, Loss: 0.2928, Time: 3.19s\n",
            "Batch 230/1018, Loss: 0.2430, Time: 3.86s\n",
            "Batch 240/1018, Loss: 0.2852, Time: 3.15s\n",
            "Batch 250/1018, Loss: 0.2509, Time: 3.43s\n",
            "Batch 260/1018, Loss: 0.2201, Time: 3.18s\n",
            "Batch 270/1018, Loss: 0.2003, Time: 3.93s\n",
            "Batch 280/1018, Loss: 0.2351, Time: 3.10s\n",
            "Batch 290/1018, Loss: 0.2128, Time: 3.54s\n",
            "Batch 300/1018, Loss: 0.2031, Time: 3.51s\n",
            "Batch 310/1018, Loss: 0.2122, Time: 4.20s\n",
            "Batch 320/1018, Loss: 0.2784, Time: 3.16s\n",
            "Batch 330/1018, Loss: 0.2176, Time: 3.45s\n",
            "Batch 340/1018, Loss: 0.2509, Time: 3.82s\n",
            "Batch 350/1018, Loss: 0.2048, Time: 3.49s\n",
            "Batch 360/1018, Loss: 0.2337, Time: 3.25s\n",
            "Batch 370/1018, Loss: 0.1876, Time: 3.17s\n",
            "Batch 380/1018, Loss: 0.2698, Time: 3.92s\n",
            "Batch 390/1018, Loss: 0.2469, Time: 3.32s\n",
            "Batch 400/1018, Loss: 0.2458, Time: 3.55s\n",
            "Batch 410/1018, Loss: 0.1789, Time: 3.53s\n",
            "Batch 420/1018, Loss: 0.2371, Time: 4.17s\n",
            "Batch 430/1018, Loss: 0.2411, Time: 3.50s\n",
            "Batch 440/1018, Loss: 0.1817, Time: 3.16s\n",
            "Batch 450/1018, Loss: 0.2566, Time: 3.56s\n",
            "Batch 460/1018, Loss: 0.2637, Time: 3.58s\n",
            "Batch 470/1018, Loss: 0.2676, Time: 3.21s\n",
            "Batch 480/1018, Loss: 0.2373, Time: 3.34s\n",
            "Batch 490/1018, Loss: 0.2028, Time: 4.14s\n",
            "Batch 500/1018, Loss: 0.2552, Time: 3.30s\n",
            "Batch 510/1018, Loss: 0.2095, Time: 3.20s\n",
            "Batch 520/1018, Loss: 0.2473, Time: 3.56s\n",
            "Batch 530/1018, Loss: 0.2347, Time: 4.14s\n",
            "Batch 540/1018, Loss: 0.2491, Time: 3.52s\n",
            "Batch 550/1018, Loss: 0.2352, Time: 3.09s\n",
            "Batch 560/1018, Loss: 0.2245, Time: 3.37s\n",
            "Batch 570/1018, Loss: 0.2558, Time: 3.57s\n",
            "Batch 580/1018, Loss: 0.2462, Time: 3.41s\n",
            "Batch 590/1018, Loss: 0.2299, Time: 3.18s\n",
            "Batch 600/1018, Loss: 0.2205, Time: 3.45s\n",
            "Batch 610/1018, Loss: 0.2548, Time: 4.34s\n",
            "Batch 620/1018, Loss: 0.2903, Time: 3.84s\n",
            "Batch 630/1018, Loss: 0.2475, Time: 3.06s\n",
            "Batch 640/1018, Loss: 0.2162, Time: 4.24s\n",
            "Batch 650/1018, Loss: 0.1997, Time: 3.42s\n",
            "Batch 660/1018, Loss: 0.2513, Time: 3.04s\n",
            "Batch 670/1018, Loss: 0.2259, Time: 3.40s\n",
            "Batch 680/1018, Loss: 0.2366, Time: 3.77s\n",
            "Batch 690/1018, Loss: 0.2535, Time: 3.60s\n",
            "Batch 700/1018, Loss: 0.2518, Time: 3.66s\n",
            "Batch 710/1018, Loss: 0.2502, Time: 3.96s\n",
            "Batch 720/1018, Loss: 0.2114, Time: 3.87s\n",
            "Batch 730/1018, Loss: 0.2687, Time: 3.87s\n",
            "Batch 740/1018, Loss: 0.2622, Time: 3.60s\n",
            "Batch 750/1018, Loss: 0.2214, Time: 4.12s\n",
            "Batch 760/1018, Loss: 0.2526, Time: 3.42s\n",
            "Batch 770/1018, Loss: 0.2622, Time: 3.39s\n",
            "Batch 780/1018, Loss: 0.2181, Time: 4.73s\n",
            "Batch 790/1018, Loss: 0.2218, Time: 3.79s\n",
            "Batch 800/1018, Loss: 0.2721, Time: 3.03s\n",
            "Batch 810/1018, Loss: 0.2233, Time: 3.40s\n",
            "Batch 820/1018, Loss: 0.2287, Time: 4.05s\n",
            "Batch 830/1018, Loss: 0.2715, Time: 3.36s\n",
            "Batch 840/1018, Loss: 0.2713, Time: 3.13s\n",
            "Batch 850/1018, Loss: 0.2339, Time: 3.37s\n",
            "Batch 860/1018, Loss: 0.2570, Time: 3.77s\n",
            "Batch 870/1018, Loss: 0.2456, Time: 3.37s\n",
            "Batch 880/1018, Loss: 0.2518, Time: 3.11s\n",
            "Batch 890/1018, Loss: 0.2198, Time: 3.15s\n",
            "Batch 900/1018, Loss: 0.2257, Time: 4.13s\n",
            "Batch 910/1018, Loss: 0.2470, Time: 3.33s\n",
            "Batch 920/1018, Loss: 0.2552, Time: 3.36s\n",
            "Batch 930/1018, Loss: 0.2727, Time: 3.94s\n",
            "Batch 940/1018, Loss: 0.2368, Time: 3.66s\n",
            "Batch 950/1018, Loss: 0.2619, Time: 3.57s\n",
            "Batch 960/1018, Loss: 0.2354, Time: 3.63s\n",
            "Batch 970/1018, Loss: 0.2258, Time: 4.05s\n",
            "Batch 980/1018, Loss: 0.2463, Time: 3.40s\n",
            "Batch 990/1018, Loss: 0.2058, Time: 3.30s\n",
            "Batch 1000/1018, Loss: 0.2497, Time: 3.70s\n",
            "Batch 1010/1018, Loss: 0.2650, Time: 4.00s\n",
            "Train Loss: 0.2366, Val Loss: 0.2348\n",
            "\n",
            "Epoch 10/10\n",
            "------------------------------\n",
            "Batch 10/1018, Loss: 0.2484, Time: 2.98s\n",
            "Batch 20/1018, Loss: 0.2441, Time: 3.96s\n",
            "Batch 30/1018, Loss: 0.3236, Time: 3.02s\n",
            "Batch 40/1018, Loss: 0.1990, Time: 3.11s\n",
            "Batch 50/1018, Loss: 0.2558, Time: 3.25s\n",
            "Batch 60/1018, Loss: 0.2434, Time: 4.48s\n",
            "Batch 70/1018, Loss: 0.3026, Time: 3.99s\n",
            "Batch 80/1018, Loss: 0.2435, Time: 3.19s\n",
            "Batch 90/1018, Loss: 0.2489, Time: 3.96s\n",
            "Batch 100/1018, Loss: 0.2490, Time: 3.37s\n",
            "Batch 110/1018, Loss: 0.2382, Time: 3.41s\n",
            "Batch 120/1018, Loss: 0.2173, Time: 3.15s\n",
            "Batch 130/1018, Loss: 0.2808, Time: 4.09s\n",
            "Batch 140/1018, Loss: 0.2532, Time: 3.40s\n",
            "Batch 150/1018, Loss: 0.2909, Time: 3.41s\n",
            "Batch 160/1018, Loss: 0.1935, Time: 3.55s\n",
            "Batch 170/1018, Loss: 0.2171, Time: 3.87s\n",
            "Batch 180/1018, Loss: 0.2965, Time: 3.19s\n",
            "Batch 190/1018, Loss: 0.2191, Time: 2.97s\n",
            "Batch 200/1018, Loss: 0.2477, Time: 3.82s\n",
            "Batch 210/1018, Loss: 0.2810, Time: 3.72s\n",
            "Batch 220/1018, Loss: 0.2551, Time: 3.13s\n",
            "Batch 230/1018, Loss: 0.2144, Time: 3.35s\n",
            "Batch 240/1018, Loss: 0.2313, Time: 5.14s\n",
            "Batch 250/1018, Loss: 0.2102, Time: 3.66s\n",
            "Batch 260/1018, Loss: 0.2393, Time: 4.29s\n",
            "Batch 270/1018, Loss: 0.2315, Time: 3.09s\n",
            "Batch 280/1018, Loss: 0.2254, Time: 4.51s\n",
            "Batch 290/1018, Loss: 0.2167, Time: 3.58s\n",
            "Batch 300/1018, Loss: 0.2058, Time: 3.07s\n",
            "Batch 310/1018, Loss: 0.2401, Time: 3.32s\n",
            "Batch 320/1018, Loss: 0.2374, Time: 4.02s\n",
            "Batch 330/1018, Loss: 0.3045, Time: 3.22s\n",
            "Batch 340/1018, Loss: 0.2312, Time: 3.71s\n",
            "Batch 350/1018, Loss: 0.2414, Time: 4.27s\n",
            "Batch 360/1018, Loss: 0.2689, Time: 3.16s\n",
            "Batch 370/1018, Loss: 0.2181, Time: 3.45s\n",
            "Batch 380/1018, Loss: 0.2575, Time: 3.21s\n",
            "Batch 390/1018, Loss: 0.2151, Time: 3.84s\n",
            "Batch 400/1018, Loss: 0.2490, Time: 3.33s\n",
            "Batch 410/1018, Loss: 0.1909, Time: 3.42s\n",
            "Batch 420/1018, Loss: 0.2169, Time: 4.26s\n",
            "Batch 430/1018, Loss: 0.2448, Time: 3.30s\n",
            "Batch 440/1018, Loss: 0.2399, Time: 3.15s\n",
            "Batch 450/1018, Loss: 0.2311, Time: 3.87s\n",
            "Batch 460/1018, Loss: 0.2837, Time: 4.01s\n",
            "Batch 470/1018, Loss: 0.2308, Time: 3.22s\n",
            "Batch 480/1018, Loss: 0.1927, Time: 3.00s\n",
            "Batch 490/1018, Loss: 0.2732, Time: 3.31s\n",
            "Batch 500/1018, Loss: 0.2817, Time: 4.34s\n",
            "Batch 510/1018, Loss: 0.1960, Time: 3.40s\n",
            "Batch 520/1018, Loss: 0.2121, Time: 3.40s\n",
            "Batch 530/1018, Loss: 0.1966, Time: 4.20s\n",
            "Batch 540/1018, Loss: 0.2125, Time: 3.66s\n",
            "Batch 550/1018, Loss: 0.2706, Time: 3.46s\n",
            "Batch 560/1018, Loss: 0.1857, Time: 3.06s\n",
            "Batch 570/1018, Loss: 0.2570, Time: 4.01s\n",
            "Batch 580/1018, Loss: 0.2105, Time: 3.20s\n",
            "Batch 590/1018, Loss: 0.2099, Time: 3.29s\n",
            "Batch 600/1018, Loss: 0.2326, Time: 3.09s\n",
            "Batch 610/1018, Loss: 0.2695, Time: 3.92s\n",
            "Batch 620/1018, Loss: 0.2364, Time: 3.21s\n",
            "Batch 630/1018, Loss: 0.2380, Time: 3.26s\n",
            "Batch 640/1018, Loss: 0.2271, Time: 3.46s\n",
            "Batch 650/1018, Loss: 0.2273, Time: 4.00s\n",
            "Batch 660/1018, Loss: 0.2364, Time: 3.15s\n",
            "Batch 670/1018, Loss: 0.2285, Time: 3.07s\n",
            "Batch 680/1018, Loss: 0.2303, Time: 3.68s\n",
            "Batch 690/1018, Loss: 0.2354, Time: 3.92s\n",
            "Batch 700/1018, Loss: 0.2210, Time: 3.49s\n",
            "Batch 710/1018, Loss: 0.2153, Time: 3.16s\n",
            "Batch 720/1018, Loss: 0.2215, Time: 4.15s\n",
            "Batch 730/1018, Loss: 0.2070, Time: 3.33s\n",
            "Batch 740/1018, Loss: 0.2685, Time: 3.60s\n",
            "Batch 750/1018, Loss: 0.1910, Time: 3.72s\n",
            "Batch 760/1018, Loss: 0.2215, Time: 3.87s\n",
            "Batch 770/1018, Loss: 0.2560, Time: 3.24s\n",
            "Batch 780/1018, Loss: 0.2421, Time: 3.46s\n",
            "Batch 790/1018, Loss: 0.2261, Time: 3.97s\n",
            "Batch 800/1018, Loss: 0.2307, Time: 4.02s\n",
            "Batch 810/1018, Loss: 0.2423, Time: 3.06s\n",
            "Batch 820/1018, Loss: 0.2329, Time: 3.32s\n",
            "Batch 830/1018, Loss: 0.2086, Time: 4.65s\n",
            "Batch 840/1018, Loss: 0.2634, Time: 3.70s\n",
            "Batch 850/1018, Loss: 0.2470, Time: 3.48s\n",
            "Batch 860/1018, Loss: 0.2579, Time: 4.18s\n",
            "Batch 870/1018, Loss: 0.3063, Time: 3.31s\n",
            "Batch 880/1018, Loss: 0.2620, Time: 3.51s\n",
            "Batch 890/1018, Loss: 0.2229, Time: 3.14s\n",
            "Batch 900/1018, Loss: 0.2280, Time: 3.66s\n",
            "Batch 910/1018, Loss: 0.2541, Time: 3.88s\n",
            "Batch 920/1018, Loss: 0.2074, Time: 3.62s\n",
            "Batch 930/1018, Loss: 0.2012, Time: 3.80s\n",
            "Batch 940/1018, Loss: 0.2071, Time: 3.74s\n",
            "Batch 950/1018, Loss: 0.2672, Time: 3.48s\n",
            "Batch 960/1018, Loss: 0.2289, Time: 3.27s\n",
            "Batch 970/1018, Loss: 0.2204, Time: 3.74s\n",
            "Batch 980/1018, Loss: 0.2523, Time: 3.67s\n",
            "Batch 990/1018, Loss: 0.2549, Time: 3.20s\n",
            "Batch 1000/1018, Loss: 0.2263, Time: 3.32s\n",
            "Batch 1010/1018, Loss: 0.1737, Time: 4.43s\n",
            "Train Loss: 0.2371, Val Loss: 0.2372\n",
            "Đã lưu mô hình tại /content/models/Pretrained_CNN_with_Attention.pth\n",
            "\n",
            "==================================================\n",
            "Đang huấn luyện Pretrained_CNN_without_Attention...\n",
            "==================================================\n",
            "\n",
            "Epoch 1/10\n",
            "------------------------------\n",
            "Batch 10/1018, Loss: 1.5109, Time: 3.30s\n",
            "Batch 20/1018, Loss: 0.7959, Time: 4.06s\n",
            "Batch 30/1018, Loss: 0.4965, Time: 3.37s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-715a81a69e82>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Câu trả lời sinh: {generated}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Gọi trực tiếp hàm train_model trong cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-715a81a69e82>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;31m# Huấn luyện\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-715a81a69e82>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-6d840a068d86>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Chuyển câu hỏi thành token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-6d840a068d86>\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}