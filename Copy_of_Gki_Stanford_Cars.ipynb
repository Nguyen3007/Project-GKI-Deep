{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Kết nối Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Đường dẫn tới thư mục dự án trên Drive\n",
        "project_dir = \"/content/drive/MyDrive/Project_Gki\"\n",
        "print(\"Nội dung trong Project_Gki:\")\n",
        "print(os.listdir(project_dir))  # Liệt kê file/thư mục trong Project_Gki\n",
        "\n",
        "# Đường dẫn tới file zip\n",
        "zip_path = os.path.join(project_dir, \"StanfordCars.zip\")\n",
        "print(\"Đường dẫn zip:\", zip_path)\n",
        "print(\"Tồn tại file zip?\", os.path.exists(zip_path))\n",
        "\n",
        "# Giải nén file zip\n",
        "if os.path.exists(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"/content/stanford_cars\")\n",
        "    print(\"Đã giải nén StanfordCars.zip\")\n",
        "else:\n",
        "    print(\"File zip không tồn tại!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EeNge8DuFhz",
        "outputId": "159f0590-46e0-46ee-b3a8-a5b85fb7e641"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Nội dung trong Project_Gki:\n",
            "['StanfordCars.zip', 'Copy of Gki_Stanford_Cars.ipynb']\n",
            "Đường dẫn zip: /content/drive/MyDrive/Project_Gki/StanfordCars.zip\n",
            "Tồn tại file zip? True\n",
            "Đã giải nén StanfordCars.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Đường dẫn đến thư mục chứa ảnh\n",
        "IMAGE_DIR = \"/content/stanford_cars/cars_train/cars_train\"\n",
        "OUTPUT_FILE = \"/content/questions_answers.json\"\n",
        "\n",
        "# Các loại câu hỏi và câu trả lời mẫu\n",
        "question_templates = [\n",
        "    \"Có bao nhiêu xe trong ảnh?\",\n",
        "    \"Màu sắc của xe là gì?\",\n",
        "    \"Đây là loại xe gì?\",\n",
        "    \"Xe này được sản xuất bởi hãng nào?\",\n",
        "    \"Xe trong ảnh đang ở đâu?\"\n",
        "]\n",
        "\n",
        "color_answers = [\"đỏ\", \"xanh dương\", \"xanh lá\", \"đen\", \"trắng\", \"bạc\", \"vàng\", \"cam\", \"xám\"]\n",
        "car_types = [\"sedan\", \"SUV\", \"xe thể thao\", \"xe bán tải\", \"xe tải\", \"xe sang\"]\n",
        "car_brands = [\"Toyota\", \"Honda\", \"BMW\", \"Mercedes\", \"Ford\", \"Audi\", \"Hyundai\", \"Kia\"]\n",
        "locations = [\"trên đường phố\", \"trong bãi đỗ xe\", \"trước tòa nhà\", \"trong garage\", \"bên đường\"]\n",
        "\n",
        "def generate_qa_pairs(image_path):\n",
        "    \"\"\"Tạo câu hỏi và câu trả lời cho một hình ảnh\"\"\"\n",
        "    filename = os.path.basename(image_path)\n",
        "    qa_pairs = []\n",
        "\n",
        "    # Câu hỏi về số lượng (giả định mỗi ảnh chỉ có một chiếc xe)\n",
        "    qa_pairs.append({\n",
        "        \"image\": filename,\n",
        "        \"question\": \"Có bao nhiêu xe trong ảnh?\",\n",
        "        \"answer\": \"Có một chiếc xe trong ảnh.\"\n",
        "    })\n",
        "\n",
        "    # Câu hỏi về màu sắc\n",
        "    color = random.choice(color_answers)\n",
        "    qa_pairs.append({\n",
        "        \"image\": filename,\n",
        "        \"question\": \"Màu sắc của xe là gì?\",\n",
        "        \"answer\": f\"Chiếc xe có màu {color}.\"\n",
        "    })\n",
        "\n",
        "    # Câu hỏi về loại xe\n",
        "    car_type = random.choice(car_types)\n",
        "    qa_pairs.append({\n",
        "        \"image\": filename,\n",
        "        \"question\": \"Đây là loại xe gì?\",\n",
        "        \"answer\": f\"Đây là một chiếc {car_type}.\"\n",
        "    })\n",
        "\n",
        "    # Câu hỏi về thương hiệu\n",
        "    brand = random.choice(car_brands)\n",
        "    qa_pairs.append({\n",
        "        \"image\": filename,\n",
        "        \"question\": \"Xe này được sản xuất bởi hãng nào?\",\n",
        "        \"answer\": f\"Đây là xe {brand}.\"\n",
        "    })\n",
        "\n",
        "    # Câu hỏi về vị trí\n",
        "    location = random.choice(locations)\n",
        "    qa_pairs.append({\n",
        "        \"image\": filename,\n",
        "        \"question\": \"Xe trong ảnh đang ở đâu?\",\n",
        "        \"answer\": f\"Chiếc xe đang ở {location}.\"\n",
        "    })\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "def create_dataset(image_dir, output_file):\n",
        "    \"\"\"Tạo bộ dữ liệu từ thư mục hình ảnh\"\"\"\n",
        "    all_qa_pairs = []\n",
        "\n",
        "    # Kiểm tra thư mục tồn tại\n",
        "    if not os.path.exists(image_dir):\n",
        "        os.makedirs(image_dir)\n",
        "        print(f\"Đã tạo thư mục {image_dir}. Vui lòng thêm các hình ảnh xe vào thư mục này.\")\n",
        "        return\n",
        "\n",
        "    # Lấy tất cả các file hình ảnh\n",
        "    image_files = [f for f in os.listdir(image_dir)\n",
        "                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    if not image_files:\n",
        "        print(f\"Không tìm thấy hình ảnh nào trong thư mục {image_dir}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Tạo bộ dữ liệu từ {len(image_files)} hình ảnh...\")\n",
        "\n",
        "    # Tạo câu hỏi và câu trả lời cho mỗi hình ảnh\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        qa_pairs = generate_qa_pairs(image_path)\n",
        "        all_qa_pairs.extend(qa_pairs)\n",
        "\n",
        "    # Lưu vào file JSON\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_qa_pairs, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Đã tạo {len(all_qa_pairs)} cặp câu hỏi-trả lời trong file {output_file}\")\n",
        "    return all_qa_pairs\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_dataset(IMAGE_DIR, OUTPUT_FILE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5u_AGQlq76u",
        "outputId": "ae7540c1-b2ad-4e97-ddb7-d5172bc0e506"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tạo bộ dữ liệu từ 8144 hình ảnh...\n",
            "Đã tạo 40720 cặp câu hỏi-trả lời trong file /content/questions_answers.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# CNN Encoder (với lựa chọn pretrained hoặc train từ đầu)\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, embed_size=512, pretrained=True):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "        if pretrained:\n",
        "            cnn = models.resnet50(pretrained=True)\n",
        "            # Đóng băng các tham số nếu sử dụng pretrained\n",
        "            for param in cnn.parameters():\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            # Train từ đầu\n",
        "            cnn = models.resnet50(pretrained=False)\n",
        "\n",
        "        self.cnn = nn.Sequential(*list(cnn.children())[:-1])\n",
        "        self.fc = nn.Linear(2048, embed_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.cnn(images)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.fc(features)\n",
        "        return features\n",
        "\n",
        "# Bộ mã hóa câu hỏi\n",
        "class QuestionEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(QuestionEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, questions):\n",
        "        embeddings = self.embedding(questions)\n",
        "        outputs, (hidden, cell) = self.lstm(embeddings)\n",
        "        features = self.fc(hidden.squeeze(0))\n",
        "        return outputs, features\n",
        "\n",
        "# LSTM Decoder với Attention\n",
        "class AttentionLSTMDecoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "        super(AttentionLSTMDecoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.attention = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.attention_combine = nn.Linear(hidden_size * 2 + embed_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, img_features, q_features, captions):\n",
        "        batch_size = captions.size(0)\n",
        "        embeddings = self.embedding(captions)\n",
        "\n",
        "        # Kết hợp đặc trưng câu hỏi với đặc trưng hình ảnh\n",
        "        combined_features = torch.cat((img_features, q_features), dim=1)\n",
        "        context = self.attention(combined_features)\n",
        "\n",
        "        # Lặp lại ngữ cảnh cho mỗi từ trong chuỗi\n",
        "        context = context.unsqueeze(1).repeat(1, embeddings.size(1), 1)\n",
        "\n",
        "        # Nối embeddings và ngữ cảnh\n",
        "        lstm_input = torch.cat((embeddings, context), dim=2)\n",
        "\n",
        "        # LSTM và đầu ra\n",
        "        hiddens, _ = self.lstm(lstm_input)\n",
        "        outputs = self.fc(hiddens)\n",
        "        return outputs\n",
        "\n",
        "# LSTM Decoder không có Attention\n",
        "class BasicLSTMDecoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "        super(BasicLSTMDecoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size + hidden_size * 2, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, img_features, q_features, captions):\n",
        "        embeddings = self.embedding(captions)\n",
        "\n",
        "        # Kết hợp đặc trưng\n",
        "        combined_features = torch.cat((img_features, q_features), dim=1)\n",
        "\n",
        "        # Lặp lại cho mỗi từ\n",
        "        features = combined_features.unsqueeze(1).repeat(1, embeddings.size(1), 1)\n",
        "        inputs = torch.cat((features, embeddings), dim=2)\n",
        "\n",
        "        # LSTM và đầu ra\n",
        "        hiddens, _ = self.lstm(inputs)\n",
        "        outputs = self.fc(hiddens)\n",
        "        return outputs\n",
        "\n",
        "# Mô hình VQA hoàn chỉnh\n",
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, use_pretrained=True, use_attention=True):\n",
        "        super(VQAModel, self).__init__()\n",
        "        self.cnn = CNNEncoder(embed_size, pretrained=use_pretrained)\n",
        "        self.question_encoder = QuestionEncoder(vocab_size, embed_size, hidden_size)\n",
        "\n",
        "        if use_attention:\n",
        "            self.decoder = AttentionLSTMDecoder(embed_size, hidden_size, vocab_size)\n",
        "        else:\n",
        "            self.decoder = BasicLSTMDecoder(embed_size, hidden_size, vocab_size)\n",
        "\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "    def forward(self, images, questions, answers=None):\n",
        "        img_features = self.cnn(images)\n",
        "        q_outputs, q_features = self.question_encoder(questions)\n",
        "\n",
        "        if answers is not None:\n",
        "            # Chế độ huấn luyện - teacher forcing\n",
        "            outputs = self.decoder(img_features, q_features, answers[:, :-1])\n",
        "            return outputs\n",
        "        else:\n",
        "            # Chế độ suy luận\n",
        "            return img_features, q_features"
      ],
      "metadata": {
        "id": "emgqfqgou55g"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# Transform ảnh\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def load_image(image_path):\n",
        "    try:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        return transform(image)\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi khi tải hình ảnh {image_path}: {e}\")\n",
        "        # Trả về tensor trống nếu không tải được hình ảnh\n",
        "        return torch.zeros(3, 224, 224)\n",
        "\n",
        "# Xây dựng từ điển\n",
        "def build_vocab(data):\n",
        "    vocab = set()\n",
        "    for item in data:\n",
        "        vocab.update(item['question'].split())\n",
        "        vocab.update(item['answer'].split())\n",
        "    vocab = ['<PAD>', '<UNK>', '<START>', '<END>'] + list(vocab)\n",
        "    return {word: idx for idx, word in enumerate(vocab)}, {idx: word for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Dataset\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, data, word2idx, image_dir):\n",
        "        self.data = data\n",
        "        self.word2idx = word2idx\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image_path = os.path.join(self.image_dir, item['image'])\n",
        "        image = load_image(image_path)\n",
        "\n",
        "        # Chuyển câu hỏi thành token\n",
        "        question = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in item['question'].split()]\n",
        "\n",
        "        # Chuyển câu trả lời thành token\n",
        "        answer = [self.word2idx['<START>']] + [self.word2idx.get(word, self.word2idx['<UNK>'])\n",
        "                                              for word in item['answer'].split()] + [self.word2idx['<END>']]\n",
        "\n",
        "        return image, torch.tensor(question), torch.tensor(answer)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, questions, answers = zip(*batch)\n",
        "    images = torch.stack(images)\n",
        "    questions = nn.utils.rnn.pad_sequence(questions, batch_first=True, padding_value=0)\n",
        "    answers = nn.utils.rnn.pad_sequence(answers, batch_first=True, padding_value=0)\n",
        "    return images, questions, answers\n",
        "\n",
        "def prepare_dataloaders(data_file, image_dir, batch_size=32):\n",
        "    # Đọc dữ liệu\n",
        "    try:\n",
        "        with open(data_file, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File dữ liệu {data_file} không tồn tại!\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    # Xây dựng từ điển\n",
        "    word2idx, idx2word = build_vocab(data)\n",
        "    vocab_size = len(word2idx)\n",
        "\n",
        "    # Chia dữ liệu\n",
        "    train_size = int(0.8 * len(data))\n",
        "    val_size = int(0.1 * len(data))\n",
        "\n",
        "    train_data = data[:train_size]\n",
        "    val_data = data[train_size:train_size + val_size]\n",
        "    test_data = data[train_size + val_size:]\n",
        "\n",
        "    # Tạo datasets\n",
        "    train_dataset = QADataset(train_data, word2idx, image_dir)\n",
        "    val_dataset = QADataset(val_data, word2idx, image_dir)\n",
        "    test_dataset = QADataset(test_data, word2idx, image_dir)\n",
        "\n",
        "    # Tạo dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=1,  # Để dễ xử lý, batch_size cho test_loader là 1\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader, word2idx, idx2word"
      ],
      "metadata": {
        "id": "5-lfM1m6JI0j"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from model import VQAModel\n",
        "from dataset import prepare_dataloaders, load_image\n",
        "\n",
        "# Cấu hình\n",
        "CONFIG = {\n",
        "    'embed_size': 256,\n",
        "    'hidden_size': 512,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 10,\n",
        "    'learning_rate': 0.001,\n",
        "    'image_dir': '/content/stanford_cars/cars_train',\n",
        "    'data_file': '/content/questions_answers.json',\n",
        "    'save_dir': '/content/models',\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "}\n",
        "\n",
        "# Tạo thư mục lưu mô hình\n",
        "if not os.path.exists(CONFIG['save_dir']):\n",
        "    os.makedirs(CONFIG['save_dir'])\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Huấn luyện mô hình qua 1 epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, (images, questions, answers) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        questions = questions.to(device)\n",
        "        answers = answers.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, questions, answers)\n",
        "\n",
        "        # Tính loss (bỏ qua padding)\n",
        "        loss = criterion(outputs.reshape(-1, outputs.size(-1)), answers[:, 1:].reshape(-1))\n",
        "\n",
        "        # Backward pass và optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # Gradient clipping\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # In tiến trình\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}, \"\n",
        "                  f\"Time: {time.time() - start_time:.2f}s\")\n",
        "            start_time = time.time()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Đánh giá mô hình trên tập validation\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, questions, answers in val_loader:\n",
        "            images = images.to(device)\n",
        "            questions = questions.to(device)\n",
        "            answers = answers.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images, questions, answers)\n",
        "\n",
        "            # Tính loss\n",
        "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), answers[:, 1:].reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "def generate_answer(model, image, question, word2idx, idx2word, device, max_len=20):\n",
        "    \"\"\"Sinh câu trả lời từ mô hình\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Chuẩn bị đầu vào\n",
        "        image = image.unsqueeze(0).to(device)\n",
        "        question = torch.tensor([word2idx.get(word, word2idx['<UNK>'])\n",
        "                                for word in question.split()]).unsqueeze(0).to(device)\n",
        "\n",
        "        # Lấy đặc trưng\n",
        "        img_features, q_features = model(image, question)\n",
        "\n",
        "        # Sinh câu trả lời\n",
        "        answer = [word2idx['<START>']]\n",
        "        for i in range(max_len):\n",
        "            # Chuyển đổi chuỗi hiện tại thành tensor\n",
        "            current_answer = torch.tensor([answer]).to(device)\n",
        "\n",
        "            # Dự đoán từ tiếp theo\n",
        "            output = model.decoder(img_features, q_features, current_answer)\n",
        "\n",
        "            # Lấy từ có khả năng cao nhất tiếp theo\n",
        "            _, next_word_idx = output[:, -1].max(1)\n",
        "            next_word_idx = next_word_idx.item()\n",
        "\n",
        "            # Thêm vào chuỗi câu trả lời\n",
        "            answer.append(next_word_idx)\n",
        "\n",
        "            # Dừng nếu gặp token <END>\n",
        "            if next_word_idx == word2idx['<END>']:\n",
        "                break\n",
        "\n",
        "        # Chuyển đổi chỉ số thành từ (bỏ qua <START> và <END>)\n",
        "        words = [idx2word[idx] for idx in answer[1:-1] if idx in idx2word]\n",
        "        return ' '.join(words)\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"Huấn luyện các biến thể của mô hình và so sánh hiệu suất\"\"\"\n",
        "    print(f\"Sử dụng thiết bị: {CONFIG['device']}\")\n",
        "\n",
        "    # Chuẩn bị dữ liệu\n",
        "    train_loader, val_loader, test_loader, word2idx, idx2word = prepare_dataloaders(\n",
        "        CONFIG['data_file'],\n",
        "        CONFIG['image_dir'],\n",
        "        CONFIG['batch_size']\n",
        "    )\n",
        "\n",
        "    if train_loader is None:\n",
        "        print(\"Không thể tải dữ liệu. Vui lòng chạy data_generation.py trước.\")\n",
        "        return\n",
        "\n",
        "    vocab_size = len(word2idx)\n",
        "    print(f\"Kích thước từ điển: {vocab_size}\")\n",
        "\n",
        "    # Định nghĩa 4 mô hình khác nhau để so sánh\n",
        "    models = {\n",
        "        \"Pretrained_CNN_with_Attention\": VQAModel(\n",
        "            embed_size=CONFIG['embed_size'],\n",
        "            hidden_size=CONFIG['hidden_size'],\n",
        "            vocab_size=vocab_size,\n",
        "            use_pretrained=True,\n",
        "            use_attention=True\n",
        "        ),\n",
        "        \"Pretrained_CNN_without_Attention\": VQAModel(\n",
        "            embed_size=CONFIG['embed_size'],\n",
        "            hidden_size=CONFIG['hidden_size'],\n",
        "            vocab_size=vocab_size,\n",
        "            use_pretrained=True,\n",
        "            use_attention=False\n",
        "        ),\n",
        "        \"From_Scratch_CNN_with_Attention\": VQAModel(\n",
        "            embed_size=CONFIG['embed_size'],\n",
        "            hidden_size=CONFIG['hidden_size'],\n",
        "            vocab_size=vocab_size,\n",
        "            use_pretrained=False,\n",
        "            use_attention=True\n",
        "        ),\n",
        "        \"From_Scratch_CNN_without_Attention\": VQAModel(\n",
        "            embed_size=CONFIG['embed_size'],\n",
        "            hidden_size=CONFIG['hidden_size'],\n",
        "            vocab_size=vocab_size,\n",
        "            use_pretrained=False,\n",
        "            use_attention=False\n",
        "        )\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Huấn luyện từng mô hình\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Đang huấn luyện {model_name}...\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        model = model.to(CONFIG['device'])\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
        "        optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(CONFIG['epochs']):\n",
        "            print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            # Huấn luyện\n",
        "            train_loss = train_epoch(model, train_loader, criterion, optimizer, CONFIG['device'])\n",
        "            train_losses.append(train_loss)\n",
        "\n",
        "            # Kiểm tra\n",
        "            val_loss = validate(model, val_loader, criterion, CONFIG['device'])\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "            # Điều chỉnh learning rate\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Lưu kết quả\n",
        "        results[model_name] = {\n",
        "            'model': model,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses\n",
        "        }\n",
        "\n",
        "        # Lưu mô hình\n",
        "        torch.save(model.state_dict(), os.path.join(CONFIG['save_dir'], f\"{model_name}.pth\"))\n",
        "        print(f\"Đã lưu mô hình tại {os.path.join(CONFIG['save_dir'], model_name)}.pth\")\n",
        "\n",
        "    # Vẽ đồ thị so sánh\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for model_name, result in results.items():\n",
        "        plt.plot(result['val_losses'], label=f\"{model_name}\")\n",
        "\n",
        "    plt.title('So sánh Loss đánh giá')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(CONFIG['save_dir'], 'model_comparison.png'))\n",
        "    print(f\"Đã lưu biểu đồ so sánh tại {os.path.join(CONFIG['save_dir'], 'model_comparison.png')}\")\n",
        "\n",
        "    # Đánh giá trên tập kiểm tra\n",
        "    print(\"\\nĐánh giá mô hình trên tập kiểm tra...\")\n",
        "\n",
        "    for model_name, result in results.items():\n",
        "        model = result['model']\n",
        "        model.eval()\n",
        "\n",
        "        print(f\"\\nMô hình: {model_name}\")\n",
        "        for i, (image, question, answer) in enumerate(test_loader):\n",
        "            if i >= 3:  # Chỉ hiển thị 3 ví dụ đầu tiên\n",
        "                break\n",
        "\n",
        "            image = image[0]\n",
        "            question_text = ' '.join([idx2word[idx.item()] for idx in question[0] if idx.item() in idx2word])\n",
        "            answer_text = ' '.join([idx2word[idx.item()] for idx in answer[0][1:-1] if idx.item() in idx2word])\n",
        "\n",
        "            generated = generate_answer(model, image, question_text, word2idx, idx2word, CONFIG['device'])\n",
        "\n",
        "            print(f\"Câu hỏi: {question_text}\")\n",
        "            print(f\"Câu trả lời thực: {answer_text}\")\n",
        "            print(f\"Câu trả lời sinh: {generated}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "yMavjmbDJQY6",
        "outputId": "75953b61-7743-4b11-c303-6850886b2eb6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-316de630fda6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVQAModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprepare_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}